{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a59cd03-60fb-40d0-8ad8-31f7912fcb27",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain.text_splitter import TokenTextSplitter\n",
    "\n",
    "text_splitter = TokenTextSplitter(chunk_size=1000, chunk_overlap=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5a9f224-0281-4f24-b00a-4d10aba20516",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at dslim/bert-base-NER were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/opt/conda/lib/python3.11/site-packages/transformers/pipelines/token_classification.py:168: UserWarning: `grouped_entities` is deprecated and will be removed in version v5.0.0, defaulted to `aggregation_strategy=\"AggregationStrategy.SIMPLE\"` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import pipeline\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"dslim/bert-base-NER\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"dslim/bert-base-NER\")\n",
    "\n",
    "nlp = pipeline(\"ner\", model=model, tokenizer=tokenizer, grouped_entities=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b51d9361-3961-4b09-a11f-bbb282ab9d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_entities(text, entity_group=\"PER\"):\n",
    "    entities = {}\n",
    "    # for i, line in enumerate(lines):\n",
    "    #     if (i % 1000 == 0):\n",
    "    #         print(f\"Processed: {i} lines\")\n",
    "        \n",
    "    results = nlp(text)\n",
    "    \n",
    "    for result in results:\n",
    "        word = text[result[\"start\"]:result[\"end\"]]\n",
    "        entity_group = result[\"entity_group\"]\n",
    "        score = result[\"score\"]\n",
    "        \n",
    "        if entity_group == \"PER\":\n",
    "            dummy = {}\n",
    "            dummy.setdefault(\"score\", 0)\n",
    "            dummy.setdefault(\"count\", 0)\n",
    "            \n",
    "            entities.setdefault(word, dummy)\n",
    "            entry = entities[word]\n",
    "            \n",
    "            if score > entry[\"score\"]:\n",
    "                entry[\"score\"] = float(round(score, 4))\n",
    "                entry[\"type\"] = entity_group\n",
    "                entry[\"count\"] += 1\n",
    "    \n",
    "                entities[word] = entry\n",
    "    \n",
    "    return entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "922a4edb-8335-4161-8eb8-6ebf70ccc6d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def entities_to_graph(entities):\n",
    "    nodes = [(k, v) for (k,v) in entities.items()]\n",
    "    edges = []\n",
    "\n",
    "    node_labels = {f\"{k}\": f\"{k}\" for k in entities}\n",
    "    edge_labels = {}\n",
    "\n",
    "    return (\n",
    "        nodes,\n",
    "        edges,\n",
    "        node_labels,\n",
    "        edge_labels\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e357d51-cb91-40a1-8a62-d2baefddbb82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import io\n",
    "from PIL import Image\n",
    "\n",
    "def visualize_graph(nodes, edges, node_labels=None, edge_labels=None, title=\"Characters and their relations\"):\n",
    "    G = nx.Graph()\n",
    "    \n",
    "    # Add nodes and edges\n",
    "    G.add_nodes_from(nodes)\n",
    "    G.add_edges_from(edges)\n",
    "    \n",
    "    # Create a figure with a reasonable size\n",
    "    plt.figure(figsize=(20, 8))\n",
    "    \n",
    "    # Set the layout for the graph\n",
    "    pos = nx.spring_layout(G, k=1, iterations=50)\n",
    "    \n",
    "    # Draw the graph\n",
    "    nx.draw_networkx_nodes(G, pos, alpha=0.7)\n",
    "    nx.draw_networkx_edges(G, pos, edge_color='gray', \n",
    "                          width=1, alpha=0.5)\n",
    "    \n",
    "    # Add node labels if provided\n",
    "    if node_labels is None:\n",
    "        node_labels = {node: str(node) for node in nodes}\n",
    "    nx.draw_networkx_labels(G, pos, node_labels)\n",
    "    \n",
    "    # Add edge labels if provided\n",
    "    if edge_labels is not None:\n",
    "        nx.draw_networkx_edge_labels(G, pos, edge_labels)\n",
    "    \n",
    "    # Add title and remove axes\n",
    "    plt.title(title)\n",
    "    plt.axis('off')\n",
    "    \n",
    "    # Show the plot\n",
    "    # plt.show()\n",
    "\n",
    "    buf = io.BytesIO()\n",
    "    plt.savefig(buf, format='png')\n",
    "    buf.seek(0)\n",
    "    plt.close()\n",
    "    img = Image.open(buf)\n",
    "    \n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0fb35d21-6935-4b22-996c-3aa11288cd35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import time\n",
    "import json\n",
    "\n",
    "def read_book(file):\n",
    "    loader = TextLoader(file)\n",
    "\n",
    "    doc = loader.load()\n",
    "    chunks = text_splitter.split_documents(doc)\n",
    "    entities = {}\n",
    "    \n",
    "    for i, chunk in enumerate(chunks):\n",
    "        new_entities = extract_entities(chunk.page_content)\n",
    "        \n",
    "        entities.update(new_entities)\n",
    "        \n",
    "        graph_data = entities_to_graph(entities)\n",
    "        graph = visualize_graph(*graph_data)\n",
    "\n",
    "        yield [\n",
    "            f\"Reading page {i+1}/{len(chunks)}. Found {len(new_entities.keys())} new characters for a total of {len(entities.keys())}\",\n",
    "            chunk.page_content,\n",
    "            graph            \n",
    "        ]\n",
    "        \n",
    "        # time.sleep(3)\n",
    "\n",
    "# demo = gr.Interface(\n",
    "#     fn=read_book,\n",
    "#     inputs=[\"file\"],\n",
    "#     outputs=[\"text\", \"text\", \"text\"],\n",
    "# )\n",
    "\n",
    "with gr.Blocks() as app:\n",
    "    gr.Markdown(\"The coolest book reading club!\")\n",
    "\n",
    "    with gr.Row(equal_height=True):\n",
    "        with gr.Column():\n",
    "            upload = gr.File(label=\"Upload a book\")\n",
    "            btn = gr.Button(value=\"Read book\")\n",
    "    \n",
    "            response = gr.Textbox(label=\"Action\")\n",
    "            \n",
    "            chapter = gr.Textbox(label=\"Current chapter\")\n",
    "            \n",
    "        with gr.Column():\n",
    "                \n",
    "            graph = gr.Image(type=\"pil\")\n",
    "    \n",
    "        btn.click(\n",
    "            fn=read_book,\n",
    "            inputs=[upload],\n",
    "            outputs=[response, chapter, graph]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f3df5783-344c-443b-b2fd-3a2b04c59342",
   "metadata": {},
   "outputs": [],
   "source": [
    "# next(read_book(\"data/dracula.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dfdcd8ea-2f44-45b1-b5d4-d592d11687a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "app.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
