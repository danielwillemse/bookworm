{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57494c08-4098-473a-9322-4e2dff8edfc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFacePipeline\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.output_parsers import PydanticOutputParser, RegexParser\n",
    "from pydantic import BaseModel, Field\n",
    "# from langchain_community.document_loaders import PyPDFLoader\n",
    "from typing import List, Dict, Any\n",
    "import json\n",
    "import torch\n",
    "import re\n",
    "import time\n",
    "\n",
    "from knowledgegraph import KnowledgeGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc877111-2b58-4f10-87e9-8ff0d31e2dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"hf.key\") as f:\n",
    "    hf_token = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a308c20c-e62d-4c2a-ba91-3280ef291fae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA mem usage: 1.0/12.5GB (8.3%)\n"
     ]
    }
   ],
   "source": [
    "BYTES_IN_GB = 1000_000_000\n",
    "\n",
    "def print_mem(msg = \"\"):\n",
    "    (free, total) = torch.cuda.mem_get_info()\n",
    "    used = total - free\n",
    "    \n",
    "    perc_usaged = round(used / total * 100.0, 1)\n",
    "    used_gb = round(used / BYTES_IN_GB, 1)\n",
    "    total_gb = round(total / BYTES_IN_GB, 1)\n",
    "    print(f'CUDA mem usage: {used_gb}/{total_gb}GB ({perc_usaged}%)')\n",
    "\n",
    "print_mem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd5df7d9-f8b0-4f60-af55-a5a951f39e20",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "badf5f9b2bad4ea197601f4a9c57d31e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.45k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43e7edd041164da2999d4f538b5c3b45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.33G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "If you want to use `BertLMHeadModel` as a standalone, add `is_decoder=True.`\n",
      "Some weights of BertLMHeadModel were not initialized from the model checkpoint at dslim/bert-large-NER and are newly initialized: ['cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d674ad593d14302bfc0b6344342dc43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/40.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "435e798fcccd4d799e38d92fd6c112a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63c98770fec9406a88b6c8ed595a85f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA mem usage: 1.3/12.5GB (10.6%)\n"
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    pipeline\n",
    ")\n",
    "\n",
    "# MODEL_NAME = \"Trelis/Mistral-7B-Instruct-v0.1-Summarize-16k\"\n",
    "MODEL_NAME= \"deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\"\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=False,\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    quantization_config=quantization_config,\n",
    "    token=hf_token\n",
    ")\n",
    "model.config.use_cache = False\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    token=hf_token\n",
    ")\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=1024,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    "    do_sample=True,\n",
    "    temperature=0.5,\n",
    "    repetition_penalty=1.2\n",
    ")\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=pipe)\n",
    "\n",
    "print_mem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "078d9f05-e0dd-48e5-a7dc-fdf0c567cc5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output should be formatted as a JSON instance that conforms to the JSON schema below.\n",
      "\n",
      "As an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\n",
      "the object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\n",
      "\n",
      "Here is the output schema:\n",
      "```\n",
      "{\"properties\": {\"people\": {\"items\": {\"type\": \"string\"}, \"title\": \"People\", \"type\": \"array\"}}, \"required\": [\"people\"]}\n",
      "```\n",
      "The output should be formatted as a JSON instance that conforms to the JSON schema below.\n",
      "\n",
      "As an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\n",
      "the object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\n",
      "\n",
      "Here is the output schema:\n",
      "```\n",
      "{\"$defs\": {\"Relation\": {\"properties\": {\"source\": {\"description\": \"Name of the source Person\", \"title\": \"Source\", \"type\": \"string\"}, \"target\": {\"description\": \"Name of the target Person\", \"title\": \"Target\", \"type\": \"string\"}, \"relations\": {\"description\": \"Nature of the relationship. E.g.: Friend, Foe, Family.\", \"title\": \"Relations\", \"type\": \"string\"}}, \"required\": [\"source\", \"target\", \"relations\"], \"title\": \"Relation\", \"type\": \"object\"}}, \"properties\": {\"relations\": {\"items\": {\"$ref\": \"#/$defs/Relation\"}, \"title\": \"Relations\", \"type\": \"array\"}}, \"required\": [\"relations\"]}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "class People(BaseModel):\n",
    "    people: List[str]\n",
    "\n",
    "class Relation(BaseModel):\n",
    "    source: str = Field(description=\"Name of the source Person\")\n",
    "    target: str = Field(description=\"Name of the target Person\")\n",
    "    relations: str = Field(description=\"Nature of the relationship. E.g.: Friend, Foe, Family.\")\n",
    "    \n",
    "class Relations(BaseModel):\n",
    "    relations: List[Relation]\n",
    "\n",
    "people_parser = PydanticOutputParser(pydantic_object=People)\n",
    "\n",
    "rel_parser = PydanticOutputParser(pydantic_object=Relations)\n",
    "\n",
    "print(people_parser.get_format_instructions())\n",
    "print(rel_parser.get_format_instructions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0e49057f-030a-40f7-9c98-50b12393f955",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KnowledgeGraphLLM:\n",
    "    def __init__(self):\n",
    "        self.kg = KnowledgeGraph()\n",
    "        self.kg.clear()\n",
    "        self.llm = llm\n",
    "        self.context_template = \"\"\"[INST] Read the given context and identify any new {subject} using only the provided context and graph data.\n",
    "            Keep your reasoning short and concise. If you don't know the answer, return an empty JSON object instead.\n",
    "\n",
    "            [GRAPH DATA]\n",
    "            {graph_data}\n",
    "            [/GRAPH_DATA]\n",
    "\n",
    "            [CONTEXT]\n",
    "            {context}\n",
    "            [/CONTEXT]\n",
    "\n",
    "            [FORMAT]\n",
    "            {format_instructions}\n",
    "            [/FORMAT]\n",
    "            \n",
    "            [/INST]\"\"\"\n",
    "        \n",
    "        # Define prompts for different operations\n",
    "        self.people_prompt = PromptTemplate(\n",
    "            input_variables=[\"context\", \"graph_data\"],\n",
    "            partial_variables={\n",
    "                \"subject\": \"people\",\n",
    "                \"format_instructions\": people_parser.get_format_instructions()\n",
    "            },\n",
    "            template=self.context_template)\n",
    "        \n",
    "        self.relation_prompt = PromptTemplate(\n",
    "            input_variables=[\"context\", \"graph_data\"],\n",
    "            partial_variables={\n",
    "                \"subject\": \"relations\",\n",
    "                \"format_instructions\": rel_parser.get_format_instructions()\n",
    "            },\n",
    "            template=self.context_template)\n",
    "        \n",
    "        self.query_prompt = PromptTemplate(\n",
    "            input_variables=[\"question\", \"graph_data\"],\n",
    "            template=\"\"\"[INST] Answer the following question using only the provided knowledge graph data.\n",
    "            If you cannot answer with certainty, say \"I cannot determine this from the available data.\"\n",
    "            \n",
    "            [GRAPH DATA]\n",
    "            {graph_data}\n",
    "            [/GRAPH_DATA]\n",
    "            \n",
    "            [QUESTION]\n",
    "            {question}\n",
    "            [/QUESTION]\n",
    "            \n",
    "            [/INST]\"\"\")\n",
    "        \n",
    "        # Create LangChain chains\n",
    "        self.people_chain = self.people_prompt | self.llm\n",
    "        self.relation_chain = self.relation_prompt | self.llm\n",
    "        self.query_chain = self.query_prompt | self.llm\n",
    "\n",
    "    def extract_people(self, context: str) -> List[Dict[str, str]]:\n",
    "        try:\n",
    "            output = self.people_chain.invoke(input={\n",
    "                \"context\": context, \n",
    "                \"graph_data\": self.graph_data()\n",
    "            })\n",
    "\n",
    "            result = re.findall(r'```json(.*?)```', output, re.DOTALL)[-1]\n",
    "\n",
    "            people = json.loads(result.strip())[\"people\"]\n",
    "            for person in people:\n",
    "                self.kg.add_node(person)\n",
    "\n",
    "            return people\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(\"Error: Could not parse LLM response as JSON\\n\")\n",
    "            return []\n",
    "\n",
    "    def extract_relationships(self, context: str) -> List[Dict[str, str]]:\n",
    "        \"\"\"Extract relationships from unstructured text using LLM.\"\"\"\n",
    "        output = \"\"\n",
    "        try:\n",
    "            # Get LLM's analysis\n",
    "            output = self.relation_chain.invoke(input={\n",
    "                \"context\": context,\n",
    "                \"graph_data\": self.graph_data()\n",
    "            })\n",
    "\n",
    "            result = re.findall(r'```json(.*?)```', output, re.DOTALL)[-1]\n",
    "            \n",
    "            relationships = json.loads(result.strip())[\"relations\"]\n",
    "            \n",
    "            # Add all extracted relationships to the knowledge graph\n",
    "            for rel in relationships:\n",
    "                self.kg.add_node(rel[\"source\"])\n",
    "                self.kg.add_node(rel[\"target\"])\n",
    "                self.kg.add_edge(rel[\"source\"], rel[\"target\"], rel[\"relation\"])\n",
    "            \n",
    "            return relationships\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(\"Error: Could not parse LLM response as JSON\\n\")\n",
    "            return []\n",
    "\n",
    "    def graph_data(self):\n",
    "        \"\"\"Dump graph data to JSON\"\"\"\n",
    "        return json.dumps(self.kg.dump(), indent=2)\n",
    "        \n",
    "    def smart_query(self, question: str) -> str:\n",
    "        \"\"\"Query the knowledge graph using LLM-powered reasoning.\"\"\"\n",
    "        # Get LLM's analysis\n",
    "        return self.query_chain.invoke(input={\n",
    "            \"question\": question,\n",
    "            \"graph_data\": self.graph_data()\n",
    "        })\n",
    "    \n",
    "    def get_graph_summary(self) -> Dict[str, Any]:\n",
    "        \"\"\"Return a summary of the knowledge graph.\"\"\"\n",
    "        return {\n",
    "            \"node_count\": len(self.kg.nodes),\n",
    "            \"edge_count\": sum(len(edges) for edges in self.kg.edges.values()),\n",
    "            \"data\": self.kg.dump()\n",
    "        }\n",
    "\n",
    "# Example usage\n",
    "def query_llm(kg_llm, contexts, subject, questions):\n",
    "    # Add information through unstructured text\n",
    "    # Extract and add relationships\n",
    "\n",
    "    for i, ctx in enumerate(contexts):\n",
    "        t0 = time.time()\n",
    "\n",
    "        people = kg_llm.extract_people(ctx)\n",
    "        relationships = kg_llm.extract_relationships(ctx)\n",
    "\n",
    "        t1 = time.time()\n",
    "        print(f'Read chapter {i} in {round(t1-t0, 1)} seconds, {round(i / len(contexts) * 100.0, 1)}% done.')\n",
    "        if i % 5 == 0:\n",
    "            print(kg_llm.graph_data())\n",
    "            torch.cuda.empty_cache()\n",
    "            print_mem()            \n",
    "\n",
    "    \n",
    "    # # Query the enhanced knowledge graph\n",
    "    # for question in questions:\n",
    "    #     answer = kg_llm.smart_query(question).partition(\"[/INST]\")[-1]\n",
    "    #     print(f\"###\\n\\nQ: {question}\\nA: {answer}\\n\\n###\")\n",
    "\n",
    "\n",
    "    \n",
    "    # Get graph summary\n",
    "    print(\"\\nKnowledge Graph Summary:\")\n",
    "    print(json.dumps(kg_llm.get_graph_summary(), indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eadcb5ba-a33d-48ce-a81a-b05f40065575",
   "metadata": {},
   "outputs": [],
   "source": [
    "snippets = [\n",
    "\"\"\"\n",
    "This is written in the train from Varna to Galatz. Last\n",
    "night we all assembled a little before the time of sunset. Each of us\n",
    "had done his work as well as he could; so far as thought, and endeavour,\n",
    "and opportunity go, we are prepared for the whole of our journey, and\n",
    "for our work when we get to Galatz. When the usual time came round Mrs.\n",
    "Harker prepared herself for her hypnotic effort; and after a longer and\n",
    "more serious effort on the part of Van Helsing than has been usually\n",
    "necessary, she sank into the trance. Usually she speaks on a hint; but\n",
    "this time the Professor had to ask her questions, and to ask them pretty\n",
    "resolutely, before we could learn anything; at last her answer came:--\n",
    "\"\"\",\n",
    "\"\"\"\n",
    "“I can see nothing; we are still; there are no waves lapping, but only a\n",
    "steady swirl of water softly running against the hawser. I can hear\n",
    "men’s voices calling, near and far, and the roll and creak of oars in\n",
    "the rowlocks. A gun is fired somewhere; the echo of it seems far away.\n",
    "There is tramping of feet overhead, and ropes and chains are dragged\n",
    "along. What is this? There is a gleam of light; I can feel the air\n",
    "blowing upon me.”\n",
    "\"\"\",\n",
    "\"\"\"\n",
    "Here she stopped. She had risen, as if impulsively, from where she lay\n",
    "on the sofa, and raised both her hands, palms upwards, as if lifting a\n",
    "weight. Van Helsing and I looked at each other with understanding.\n",
    "Quincey raised his eyebrows slightly and looked at her intently, whilst\n",
    "Harker’s hand instinctively closed round the hilt of his Kukri. There\n",
    "was a long pause. We all knew that the time when she could speak was\n",
    "passing; but we felt that it was useless to say anything. Suddenly she\n",
    "sat up, and, as she opened her eyes, said sweetly:--\n",
    "\"\"\",\n",
    "\"\"\"\n",
    "“Would none of you like a cup of tea? You must all be so tired!” We\n",
    "could only make her happy, and so acquiesced. She bustled off to get\n",
    "tea; when she had gone Van Helsing said:--\n",
    "\"\"\",\n",
    "\"\"\"\n",
    "“You see, my friends. _He_ is close to land: he has left his\n",
    "earth-chest. But he has yet to get on shore. In the night he may lie\n",
    "hidden somewhere; but if he be not carried on shore, or if the ship do\n",
    "not touch it, he cannot achieve the land. In such case he can, if it be\n",
    "in the night, change his form and can jump or fly on shore, as he did\n",
    "at Whitby. But if the day come before he get on shore, then, unless he\n",
    "be carried he cannot escape. And if he be carried, then the customs men\n",
    "may discover what the box contain. Thus, in fine, if he escape not on\n",
    "shore to-night, or before dawn, there will be the whole day lost to him.\n",
    "We may then arrive in time; for if he escape not at night we shall come\n",
    "on him in daytime, boxed up and at our mercy; for he dare not be his\n",
    "true self, awake and visible, lest he be discovered.”\n",
    "\"\"\"\n",
    "]\n",
    "questions = []\n",
    "\n",
    "# query_llm(KnowledgeGraphLLM(), snippets, \"Dracula\", questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "df76606c-388e-4bbe-aadf-95fd02758ad6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA mem usage: 9.2/12.5GB (73.6%)\n"
     ]
    }
   ],
   "source": [
    "print_mem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "334ed151-b828-4056-a0c1-e5006b92ef31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_book():\n",
    "    # Initialize the system\n",
    "    kg_llm = KnowledgeGraphLLM()\n",
    "    \n",
    "    with open(\"data/dracula.txt\") as f:\n",
    "        context = f.read()\n",
    "        context = context.rpartition(r\"\\n\\nDRACULA\\n\\n\")[-1]\n",
    "\n",
    "    paragraphs = []\n",
    "    chapters = re.split(r\"\\n\\nCHAPTER\\ .*\\n\", context)\n",
    "    for c in chapters[:5]:\n",
    "        for p in re.split(r\"\\n\\n\", c):\n",
    "            paragraphs.append(p)\n",
    "\n",
    "    questions = [\n",
    "        # \"Who are the main characters in this story?\",\n",
    "        # \"What is their relation to each other?\",\n",
    "        # \"Who is the antagonist of the story?\",\n",
    "        # \"How does the antagonist end up?\"\n",
    "    ]\n",
    "    \n",
    "    query_llm(kg_llm, paragraphs, \"Dracula\", questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b57e1c04-184b-4c6c-a776-c9bd4850b66a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Read chapter 0 in 0.0 seconds, 0.0% done.\n",
      "{\n",
      "  \"people\": {},\n",
      "  \"relations\": {}\n",
      "}\n",
      "CUDA mem usage: 9.2/12.5GB (73.6%)\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Read chapter 1 in 0.0 seconds, 0.6% done.\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Read chapter 2 in 0.0 seconds, 1.1% done.\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Read chapter 3 in 0.0 seconds, 1.7% done.\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Read chapter 4 in 0.0 seconds, 2.2% done.\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Read chapter 5 in 0.0 seconds, 2.8% done.\n",
      "{\n",
      "  \"people\": {},\n",
      "  \"relations\": {}\n",
      "}\n",
      "CUDA mem usage: 9.2/12.5GB (73.6%)\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Read chapter 6 in 0.0 seconds, 3.3% done.\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Read chapter 7 in 0.0 seconds, 3.9% done.\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Read chapter 8 in 0.0 seconds, 4.4% done.\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Read chapter 9 in 0.0 seconds, 5.0% done.\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Read chapter 10 in 0.0 seconds, 5.5% done.\n",
      "{\n",
      "  \"people\": {},\n",
      "  \"relations\": {}\n",
      "}\n",
      "CUDA mem usage: 9.2/12.5GB (73.6%)\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Read chapter 11 in 0.0 seconds, 6.1% done.\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Read chapter 12 in 0.0 seconds, 6.6% done.\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Read chapter 13 in 0.0 seconds, 7.2% done.\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Read chapter 14 in 0.0 seconds, 7.7% done.\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Read chapter 15 in 0.0 seconds, 8.3% done.\n",
      "{\n",
      "  \"people\": {},\n",
      "  \"relations\": {}\n",
      "}\n",
      "CUDA mem usage: 9.2/12.5GB (73.6%)\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Read chapter 16 in 0.0 seconds, 8.8% done.\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Read chapter 17 in 0.0 seconds, 9.4% done.\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Read chapter 18 in 0.0 seconds, 9.9% done.\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Read chapter 19 in 0.0 seconds, 10.5% done.\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Read chapter 20 in 0.0 seconds, 11.0% done.\n",
      "{\n",
      "  \"people\": {},\n",
      "  \"relations\": {}\n",
      "}\n",
      "CUDA mem usage: 9.2/12.5GB (73.6%)\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Read chapter 21 in 0.0 seconds, 11.6% done.\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Read chapter 22 in 0.0 seconds, 12.2% done.\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Read chapter 23 in 0.0 seconds, 12.7% done.\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Read chapter 24 in 0.0 seconds, 13.3% done.\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Read chapter 25 in 0.0 seconds, 13.8% done.\n",
      "{\n",
      "  \"people\": {},\n",
      "  \"relations\": {}\n",
      "}\n",
      "CUDA mem usage: 9.2/12.5GB (73.6%)\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Read chapter 26 in 0.0 seconds, 14.4% done.\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Read chapter 27 in 0.0 seconds, 14.9% done.\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Read chapter 28 in 0.0 seconds, 15.5% done.\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Read chapter 29 in 0.0 seconds, 16.0% done.\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Read chapter 30 in 0.0 seconds, 16.6% done.\n",
      "{\n",
      "  \"people\": {},\n",
      "  \"relations\": {}\n",
      "}\n",
      "CUDA mem usage: 9.2/12.5GB (73.6%)\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Read chapter 31 in 0.0 seconds, 17.1% done.\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Read chapter 32 in 0.0 seconds, 17.7% done.\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Read chapter 33 in 0.0 seconds, 18.2% done.\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Read chapter 34 in 0.0 seconds, 18.8% done.\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Read chapter 35 in 0.0 seconds, 19.3% done.\n",
      "{\n",
      "  \"people\": {},\n",
      "  \"relations\": {}\n",
      "}\n",
      "CUDA mem usage: 9.2/12.5GB (73.6%)\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Read chapter 36 in 0.0 seconds, 19.9% done.\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Read chapter 37 in 0.0 seconds, 20.4% done.\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Read chapter 38 in 0.0 seconds, 21.0% done.\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Read chapter 39 in 0.0 seconds, 21.5% done.\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Read chapter 40 in 0.0 seconds, 22.1% done.\n",
      "{\n",
      "  \"people\": {},\n",
      "  \"relations\": {}\n",
      "}\n",
      "CUDA mem usage: 9.2/12.5GB (73.6%)\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Read chapter 41 in 0.0 seconds, 22.7% done.\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Read chapter 42 in 0.0 seconds, 23.2% done.\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Read chapter 43 in 0.0 seconds, 23.8% done.\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Read chapter 44 in 0.0 seconds, 24.3% done.\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Read chapter 45 in 0.0 seconds, 24.9% done.\n",
      "{\n",
      "  \"people\": {},\n",
      "  \"relations\": {}\n",
      "}\n",
      "CUDA mem usage: 9.2/12.5GB (73.6%)\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Read chapter 46 in 0.0 seconds, 25.4% done.\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Read chapter 47 in 0.0 seconds, 26.0% done.\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Read chapter 48 in 0.0 seconds, 26.5% done.\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Read chapter 49 in 0.0 seconds, 27.1% done.\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Read chapter 50 in 0.0 seconds, 27.6% done.\n",
      "{\n",
      "  \"people\": {},\n",
      "  \"relations\": {}\n",
      "}\n",
      "CUDA mem usage: 9.2/12.5GB (73.6%)\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Read chapter 51 in 0.0 seconds, 28.2% done.\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Read chapter 52 in 0.0 seconds, 28.7% done.\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Read chapter 53 in 0.0 seconds, 29.3% done.\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Read chapter 54 in 0.0 seconds, 29.8% done.\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Read chapter 55 in 0.0 seconds, 30.4% done.\n",
      "{\n",
      "  \"people\": {},\n",
      "  \"relations\": {}\n",
      "}\n",
      "CUDA mem usage: 9.2/12.5GB (73.6%)\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Read chapter 56 in 0.0 seconds, 30.9% done.\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Read chapter 57 in 0.0 seconds, 31.5% done.\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Read chapter 58 in 0.0 seconds, 32.0% done.\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Read chapter 59 in 0.0 seconds, 32.6% done.\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Read chapter 60 in 0.0 seconds, 33.1% done.\n",
      "{\n",
      "  \"people\": {},\n",
      "  \"relations\": {}\n",
      "}\n",
      "CUDA mem usage: 9.2/12.5GB (73.6%)\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Read chapter 61 in 0.0 seconds, 33.7% done.\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Read chapter 62 in 0.0 seconds, 34.3% done.\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Read chapter 63 in 0.0 seconds, 34.8% done.\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Read chapter 64 in 0.0 seconds, 35.4% done.\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Read chapter 65 in 0.0 seconds, 35.9% done.\n",
      "{\n",
      "  \"people\": {},\n",
      "  \"relations\": {}\n",
      "}\n",
      "CUDA mem usage: 9.2/12.5GB (73.6%)\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Read chapter 66 in 0.0 seconds, 36.5% done.\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Read chapter 67 in 0.0 seconds, 37.0% done.\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Read chapter 68 in 0.0 seconds, 37.6% done.\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Read chapter 69 in 0.0 seconds, 38.1% done.\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Read chapter 70 in 0.0 seconds, 38.7% done.\n",
      "{\n",
      "  \"people\": {},\n",
      "  \"relations\": {}\n",
      "}\n",
      "CUDA mem usage: 9.2/12.5GB (73.6%)\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Read chapter 71 in 0.0 seconds, 39.2% done.\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Read chapter 72 in 0.0 seconds, 39.8% done.\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Read chapter 73 in 0.0 seconds, 40.3% done.\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Read chapter 74 in 0.0 seconds, 40.9% done.\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Read chapter 75 in 0.0 seconds, 41.4% done.\n",
      "{\n",
      "  \"people\": {},\n",
      "  \"relations\": {}\n",
      "}\n",
      "CUDA mem usage: 9.2/12.5GB (73.6%)\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Read chapter 76 in 0.0 seconds, 42.0% done.\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Read chapter 77 in 0.0 seconds, 42.5% done.\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Read chapter 78 in 0.0 seconds, 43.1% done.\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Read chapter 79 in 0.0 seconds, 43.6% done.\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Read chapter 80 in 0.0 seconds, 44.2% done.\n",
      "{\n",
      "  \"people\": {},\n",
      "  \"relations\": {}\n",
      "}\n",
      "CUDA mem usage: 9.2/12.5GB (73.6%)\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Read chapter 81 in 0.0 seconds, 44.8% done.\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Read chapter 82 in 0.0 seconds, 45.3% done.\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Read chapter 83 in 0.0 seconds, 45.9% done.\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Read chapter 84 in 0.0 seconds, 46.4% done.\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Read chapter 85 in 0.0 seconds, 47.0% done.\n",
      "{\n",
      "  \"people\": {},\n",
      "  \"relations\": {}\n",
      "}\n",
      "CUDA mem usage: 9.2/12.5GB (73.6%)\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Read chapter 86 in 0.0 seconds, 47.5% done.\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Read chapter 87 in 0.0 seconds, 48.1% done.\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Read chapter 88 in 0.0 seconds, 48.6% done.\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Read chapter 89 in 0.0 seconds, 49.2% done.\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Read chapter 90 in 0.0 seconds, 49.7% done.\n",
      "{\n",
      "  \"people\": {},\n",
      "  \"relations\": {}\n",
      "}\n",
      "CUDA mem usage: 9.2/12.5GB (73.6%)\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Read chapter 91 in 0.0 seconds, 50.3% done.\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Read chapter 92 in 0.0 seconds, 50.8% done.\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Read chapter 93 in 0.0 seconds, 51.4% done.\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Read chapter 94 in 0.0 seconds, 51.9% done.\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Read chapter 95 in 0.0 seconds, 52.5% done.\n",
      "{\n",
      "  \"people\": {},\n",
      "  \"relations\": {}\n",
      "}\n",
      "CUDA mem usage: 9.2/12.5GB (73.6%)\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Read chapter 96 in 0.0 seconds, 53.0% done.\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Read chapter 97 in 0.0 seconds, 53.6% done.\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Read chapter 98 in 0.0 seconds, 54.1% done.\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Read chapter 99 in 0.0 seconds, 54.7% done.\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Read chapter 100 in 0.0 seconds, 55.2% done.\n",
      "{\n",
      "  \"people\": {},\n",
      "  \"relations\": {}\n",
      "}\n",
      "CUDA mem usage: 9.2/12.5GB (73.6%)\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Read chapter 101 in 0.0 seconds, 55.8% done.\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Read chapter 102 in 0.0 seconds, 56.4% done.\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Read chapter 103 in 0.0 seconds, 56.9% done.\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Read chapter 104 in 0.0 seconds, 57.5% done.\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Read chapter 105 in 0.0 seconds, 58.0% done.\n",
      "{\n",
      "  \"people\": {},\n",
      "  \"relations\": {}\n",
      "}\n",
      "CUDA mem usage: 9.2/12.5GB (73.6%)\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Read chapter 106 in 0.0 seconds, 58.6% done.\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Read chapter 107 in 0.0 seconds, 59.1% done.\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Read chapter 108 in 0.0 seconds, 59.7% done.\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Read chapter 109 in 0.0 seconds, 60.2% done.\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Read chapter 110 in 0.0 seconds, 60.8% done.\n",
      "{\n",
      "  \"people\": {},\n",
      "  \"relations\": {}\n",
      "}\n",
      "CUDA mem usage: 9.2/12.5GB (73.6%)\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Read chapter 111 in 0.0 seconds, 61.3% done.\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Read chapter 112 in 0.0 seconds, 61.9% done.\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Read chapter 113 in 0.0 seconds, 62.4% done.\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Read chapter 114 in 0.0 seconds, 63.0% done.\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Read chapter 115 in 0.0 seconds, 63.5% done.\n",
      "{\n",
      "  \"people\": {},\n",
      "  \"relations\": {}\n",
      "}\n",
      "CUDA mem usage: 9.2/12.5GB (73.6%)\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Read chapter 116 in 0.0 seconds, 64.1% done.\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Read chapter 117 in 0.0 seconds, 64.6% done.\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Read chapter 118 in 0.0 seconds, 65.2% done.\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Read chapter 119 in 0.0 seconds, 65.7% done.\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Read chapter 120 in 0.0 seconds, 66.3% done.\n",
      "{\n",
      "  \"people\": {},\n",
      "  \"relations\": {}\n",
      "}\n",
      "CUDA mem usage: 9.2/12.5GB (73.6%)\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Read chapter 121 in 0.0 seconds, 66.9% done.\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Read chapter 122 in 0.0 seconds, 67.4% done.\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Read chapter 123 in 0.0 seconds, 68.0% done.\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Read chapter 124 in 0.0 seconds, 68.5% done.\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Read chapter 125 in 0.0 seconds, 69.1% done.\n",
      "{\n",
      "  \"people\": {},\n",
      "  \"relations\": {}\n",
      "}\n",
      "CUDA mem usage: 9.2/12.5GB (73.6%)\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Read chapter 126 in 0.0 seconds, 69.6% done.\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Read chapter 127 in 0.0 seconds, 70.2% done.\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Read chapter 128 in 0.0 seconds, 70.7% done.\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Read chapter 129 in 0.0 seconds, 71.3% done.\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Read chapter 130 in 0.0 seconds, 71.8% done.\n",
      "{\n",
      "  \"people\": {},\n",
      "  \"relations\": {}\n",
      "}\n",
      "CUDA mem usage: 9.2/12.5GB (73.6%)\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Read chapter 131 in 0.0 seconds, 72.4% done.\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Read chapter 132 in 0.0 seconds, 72.9% done.\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Read chapter 133 in 0.0 seconds, 73.5% done.\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Read chapter 134 in 0.0 seconds, 74.0% done.\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Read chapter 135 in 0.0 seconds, 74.6% done.\n",
      "{\n",
      "  \"people\": {},\n",
      "  \"relations\": {}\n",
      "}\n",
      "CUDA mem usage: 9.2/12.5GB (73.6%)\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Read chapter 136 in 0.0 seconds, 75.1% done.\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Read chapter 137 in 0.0 seconds, 75.7% done.\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Read chapter 138 in 0.0 seconds, 76.2% done.\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Read chapter 139 in 0.0 seconds, 76.8% done.\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Read chapter 140 in 0.0 seconds, 77.3% done.\n",
      "{\n",
      "  \"people\": {},\n",
      "  \"relations\": {}\n",
      "}\n",
      "CUDA mem usage: 9.2/12.5GB (73.6%)\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Read chapter 141 in 0.0 seconds, 77.9% done.\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Read chapter 142 in 0.0 seconds, 78.5% done.\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Read chapter 143 in 0.0 seconds, 79.0% done.\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Read chapter 144 in 0.0 seconds, 79.6% done.\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Read chapter 145 in 0.0 seconds, 80.1% done.\n",
      "{\n",
      "  \"people\": {},\n",
      "  \"relations\": {}\n",
      "}\n",
      "CUDA mem usage: 9.2/12.5GB (73.6%)\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Read chapter 146 in 0.0 seconds, 80.7% done.\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Read chapter 147 in 0.0 seconds, 81.2% done.\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Read chapter 148 in 0.0 seconds, 81.8% done.\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Read chapter 149 in 0.0 seconds, 82.3% done.\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Read chapter 150 in 0.0 seconds, 82.9% done.\n",
      "{\n",
      "  \"people\": {},\n",
      "  \"relations\": {}\n",
      "}\n",
      "CUDA mem usage: 9.2/12.5GB (73.6%)\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Read chapter 151 in 0.0 seconds, 83.4% done.\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Read chapter 152 in 0.0 seconds, 84.0% done.\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Read chapter 153 in 0.0 seconds, 84.5% done.\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Read chapter 154 in 0.0 seconds, 85.1% done.\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Read chapter 155 in 0.0 seconds, 85.6% done.\n",
      "{\n",
      "  \"people\": {},\n",
      "  \"relations\": {}\n",
      "}\n",
      "CUDA mem usage: 9.2/12.5GB (73.6%)\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Read chapter 156 in 0.0 seconds, 86.2% done.\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Read chapter 157 in 0.0 seconds, 86.7% done.\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Read chapter 158 in 0.0 seconds, 87.3% done.\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Read chapter 159 in 0.0 seconds, 87.8% done.\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Read chapter 160 in 0.0 seconds, 88.4% done.\n",
      "{\n",
      "  \"people\": {},\n",
      "  \"relations\": {}\n",
      "}\n",
      "CUDA mem usage: 9.2/12.5GB (73.6%)\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Read chapter 161 in 0.0 seconds, 89.0% done.\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Read chapter 162 in 0.0 seconds, 89.5% done.\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Read chapter 163 in 0.0 seconds, 90.1% done.\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Read chapter 164 in 0.0 seconds, 90.6% done.\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Read chapter 165 in 0.0 seconds, 91.2% done.\n",
      "{\n",
      "  \"people\": {},\n",
      "  \"relations\": {}\n",
      "}\n",
      "CUDA mem usage: 9.2/12.5GB (73.6%)\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Read chapter 166 in 0.0 seconds, 91.7% done.\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Read chapter 167 in 0.0 seconds, 92.3% done.\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Read chapter 168 in 0.0 seconds, 92.8% done.\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Read chapter 169 in 0.0 seconds, 93.4% done.\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Read chapter 170 in 0.0 seconds, 93.9% done.\n",
      "{\n",
      "  \"people\": {},\n",
      "  \"relations\": {}\n",
      "}\n",
      "CUDA mem usage: 9.2/12.5GB (73.6%)\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Read chapter 171 in 0.0 seconds, 94.5% done.\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Read chapter 172 in 0.0 seconds, 95.0% done.\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Read chapter 173 in 0.0 seconds, 95.6% done.\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Read chapter 174 in 0.0 seconds, 96.1% done.\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Read chapter 175 in 0.0 seconds, 96.7% done.\n",
      "{\n",
      "  \"people\": {},\n",
      "  \"relations\": {}\n",
      "}\n",
      "CUDA mem usage: 9.2/12.5GB (73.6%)\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Read chapter 176 in 0.0 seconds, 97.2% done.\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Read chapter 177 in 0.0 seconds, 97.8% done.\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Read chapter 178 in 0.0 seconds, 98.3% done.\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Read chapter 179 in 0.0 seconds, 98.9% done.\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Error: Could not parse LLM response as JSON\n",
      "\n",
      "Read chapter 180 in 0.0 seconds, 99.4% done.\n",
      "{\n",
      "  \"people\": {},\n",
      "  \"relations\": {}\n",
      "}\n",
      "CUDA mem usage: 9.2/12.5GB (73.6%)\n",
      "\n",
      "Knowledge Graph Summary:\n",
      "{\n",
      "  \"node_count\": 0,\n",
      "  \"edge_count\": 0,\n",
      "  \"data\": {\n",
      "    \"people\": {},\n",
      "    \"relations\": {}\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "parse_book()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef7dcd0-6433-4d5c-9e82-246b42f31b84",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
