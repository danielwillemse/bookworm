{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57494c08-4098-473a-9322-4e2dff8edfc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFacePipeline\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.output_parsers import PydanticOutputParser, RegexParser\n",
    "from pydantic import BaseModel, Field\n",
    "# from langchain_community.document_loaders import PyPDFLoader\n",
    "from typing import List, Dict, Any\n",
    "import json\n",
    "import torch\n",
    "import re\n",
    "\n",
    "from knowledgegraph import KnowledgeGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc877111-2b58-4f10-87e9-8ff0d31e2dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"hf.key\") as f:\n",
    "    hf_token = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a308c20c-e62d-4c2a-ba91-3280ef291fae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA mem usage: 1.1/12.5GB (9.2%)\n"
     ]
    }
   ],
   "source": [
    "BYTES_IN_GB = 1000_000_000\n",
    "\n",
    "def print_mem(msg = \"\"):\n",
    "    (free, total) = torch.cuda.mem_get_info()\n",
    "    used = total - free\n",
    "    \n",
    "    perc_usaged = round(used / total * 100.0, 1)\n",
    "    used_gb = round(used / BYTES_IN_GB, 1)\n",
    "    total_gb = round(total / BYTES_IN_GB, 1)\n",
    "    print(f'CUDA mem usage: {used_gb}/{total_gb}GB ({perc_usaged}%)')\n",
    "\n",
    "print_mem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd5df7d9-f8b0-4f60-af55-a5a951f39e20",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d74e110bf7742e5afab6a4096284860",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA mem usage: 9.2/12.5GB (73.5%)\n"
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    pipeline\n",
    ")\n",
    "\n",
    "# MODEL_NAME = \"Trelis/Mistral-7B-Instruct-v0.1-Summarize-16k\"\n",
    "MODEL_NAME= \"deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\"\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=False,\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    quantization_config=quantization_config,\n",
    "    token=hf_token\n",
    ")\n",
    "model.config.use_cache = False\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    token=hf_token\n",
    ")\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=1024,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    "    do_sample=True,\n",
    "    temperature=0.1,\n",
    "    repetition_penalty=1.2\n",
    ")\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=pipe)\n",
    "\n",
    "print_mem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "078d9f05-e0dd-48e5-a7dc-fdf0c567cc5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output should be formatted as a JSON instance that conforms to the JSON schema below.\n",
      "\n",
      "As an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\n",
      "the object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\n",
      "\n",
      "Here is the output schema:\n",
      "```\n",
      "{\"properties\": {\"people\": {\"items\": {\"type\": \"string\"}, \"title\": \"People\", \"type\": \"array\"}}, \"required\": [\"people\"]}\n",
      "```\n",
      "The output should be formatted as a JSON instance that conforms to the JSON schema below.\n",
      "\n",
      "As an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\n",
      "the object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\n",
      "\n",
      "Here is the output schema:\n",
      "```\n",
      "{\"$defs\": {\"Relation\": {\"properties\": {\"source\": {\"description\": \"Name of the source Person\", \"title\": \"Source\", \"type\": \"string\"}, \"target\": {\"description\": \"Name of the target Person\", \"title\": \"Target\", \"type\": \"string\"}, \"relations\": {\"description\": \"Nature of the relationship. E.g.: Friend, Foe, Family.\", \"title\": \"Relations\", \"type\": \"string\"}}, \"required\": [\"source\", \"target\", \"relations\"], \"title\": \"Relation\", \"type\": \"object\"}}, \"properties\": {\"relations\": {\"items\": {\"$ref\": \"#/$defs/Relation\"}, \"title\": \"Relations\", \"type\": \"array\"}}, \"required\": [\"relations\"]}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "class People(BaseModel):\n",
    "    people: List[str]\n",
    "\n",
    "class Relation(BaseModel):\n",
    "    source: str = Field(description=\"Name of the source Person\")\n",
    "    target: str = Field(description=\"Name of the target Person\")\n",
    "    relations: str = Field(description=\"Nature of the relationship. E.g.: Friend, Foe, Family.\")\n",
    "    \n",
    "class Relations(BaseModel):\n",
    "    relations: List[Relation]\n",
    "\n",
    "people_parser = PydanticOutputParser(pydantic_object=People)\n",
    "\n",
    "rel_parser = PydanticOutputParser(pydantic_object=Relations)\n",
    "\n",
    "print(people_parser.get_format_instructions())\n",
    "print(rel_parser.get_format_instructions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0e49057f-030a-40f7-9c98-50b12393f955",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KnowledgeGraphLLM:\n",
    "    def __init__(self):\n",
    "        self.kg = KnowledgeGraph()\n",
    "        self.kg.clear()\n",
    "        self.llm = llm\n",
    "        \n",
    "        # Define prompts for different operations\n",
    "        self.people_prompt = PromptTemplate(\n",
    "            input_variables=[\"context\"],\n",
    "            partial_variables={\n",
    "                \"format_instructions\": people_parser.get_format_instructions()\n",
    "            },\n",
    "            template=\"\"\"[INST] Read the given context and identify any people.\n",
    "            Keep your reasoning short and concise. If you don't know the answer, return an empty JSON object instead.\n",
    "\n",
    "            [CONTEXT]\n",
    "            {context}\n",
    "            [/CONTEXT]\n",
    "\n",
    "            [FORMAT]\n",
    "            {format_instructions}\n",
    "            [/FORMAT]\n",
    "            \n",
    "            [/INST]\"\"\")\n",
    "        \n",
    "        self.relation_prompt = PromptTemplate(\n",
    "            input_variables=[\"context\"],\n",
    "            partial_variables={\n",
    "                \"format_instructions\": rel_parser.get_format_instructions()\n",
    "            },\n",
    "            template=\"\"\"[INST] Read the given context and identify any relations using only the provided knowledge graph data.\n",
    "            Keep your reasoning short and concise. If you don't know the answer, return an empty JSON object instead.\n",
    "\n",
    "            [GRAPH DATA]\n",
    "            {graph_data}\n",
    "            [/GRAPH_DATA]\n",
    "        \n",
    "            [CONTEXT]\n",
    "            {context}\n",
    "            [/CONTEXT]\n",
    "\n",
    "            [FORMAT]\n",
    "            {format_instructions}\n",
    "            [/FORMAT]\n",
    "        \n",
    "            [/INST]\"\"\")\n",
    "        \n",
    "        self.query_prompt = PromptTemplate(\n",
    "            input_variables=[\"question\", \"graph_data\"],\n",
    "            template=\"\"\"[INST] Answer the following question using only the provided knowledge graph data.\n",
    "            If you cannot answer with certainty, say \"I cannot determine this from the available data.\"\n",
    "            \n",
    "            [GRAPH DATA]\n",
    "            {graph_data}\n",
    "            [/GRAPH_DATA]\n",
    "            \n",
    "            [QUESTION]\n",
    "            {question}\n",
    "            [/QUESTION]\n",
    "            \n",
    "            [/INST]\"\"\")\n",
    "        \n",
    "        # Create LangChain chains\n",
    "        self.people_chain = self.people_prompt | self.llm\n",
    "        self.relation_chain = self.relation_prompt | self.llm\n",
    "        self.query_chain = self.query_prompt | self.llm\n",
    "\n",
    "    def extract_people(self, subject: str, context):\n",
    "        try:\n",
    "            output = self.people_chain.invoke(input={\n",
    "              \"context\": context\n",
    "            })\n",
    "\n",
    "            result = re.findall(r'```json(.*?)```', output, re.DOTALL)[-1]\n",
    "\n",
    "            people = json.loads(result.strip())[\"people\"]\n",
    "            for person in people:\n",
    "                self.kg.add_node(person)\n",
    "\n",
    "            return people\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(\"Error: Could not parse LLM response as JSON\\n\")\n",
    "            print(e, '\\n\\n')\n",
    "            print('>', output, '<')\n",
    "            return []\n",
    "\n",
    "    def extract_relationships(self, subject: str, context: str) -> List[Dict[str, str]]:\n",
    "        \"\"\"Extract relationships from unstructured text using LLM.\"\"\"\n",
    "        output = \"\"\n",
    "        try:\n",
    "            # Get LLM's analysis\n",
    "            output = self.relation_chain.invoke(input={\n",
    "                \"context\": context, \n",
    "                \"graph_data\": self.graph_data()\n",
    "            })\n",
    "\n",
    "            result = re.findall(r'```json(.*?)```', output, re.DOTALL)[-1]\n",
    "            \n",
    "            relationships = json.loads(result.strip())[\"relations\"]\n",
    "            \n",
    "            # Add all extracted relationships to the knowledge graph\n",
    "            for rel in relationships:\n",
    "                self.kg.add_node(rel[\"source\"])\n",
    "                self.kg.add_node(rel[\"target\"])\n",
    "                self.kg.add_edge(rel[\"source\"], rel[\"target\"], rel[\"relation\"])\n",
    "            \n",
    "            return relationships\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(\"Error: Could not parse LLM response as JSON\\n\")\n",
    "            print(e, '\\n\\n')\n",
    "            print('>', output, '<')\n",
    "            return []\n",
    "\n",
    "    def graph_data(self):\n",
    "        \"\"\"Dump graph data to JSON\"\"\"\n",
    "        return json.dumps(self.kg.dump(), indent=2)\n",
    "        \n",
    "    def smart_query(self, question: str) -> str:\n",
    "        \"\"\"Query the knowledge graph using LLM-powered reasoning.\"\"\"\n",
    "        # Get LLM's analysis\n",
    "        return self.query_chain.invoke(input={\n",
    "            \"question\": question,\n",
    "            \"graph_data\": self.graph_data()\n",
    "        })\n",
    "    \n",
    "    def get_graph_summary(self) -> Dict[str, Any]:\n",
    "        \"\"\"Return a summary of the knowledge graph.\"\"\"\n",
    "        return {\n",
    "            \"node_count\": len(self.kg.nodes),\n",
    "            \"edge_count\": sum(len(edges) for edges in self.kg.edges.values()),\n",
    "            \"data\": self.kg.dump()\n",
    "        }\n",
    "\n",
    "# Example usage\n",
    "def query_llm(kg_llm, contexts, subject, questions):\n",
    "    # Add information through unstructured text\n",
    "    # Extract and add relationships\n",
    "\n",
    "    for i, ctx in enumerate(contexts):\n",
    "        people = kg_llm.extract_people(subject, ctx)\n",
    "    #     relationships = kg_llm.extract_relationships(subject, ctx)\n",
    "    #     print(f\"\\r\\nExtracted data for ctx#{i}\\n{ctx[:50]}:\\n\")\n",
    "    #     print(json.dumps({\"people\": people, \"relationships\": relationships}, indent=2))\n",
    "    \n",
    "    # # Query the enhanced knowledge graph\n",
    "    # for question in questions:\n",
    "    #     answer = kg_llm.smart_query(question).partition(\"[/INST]\")[-1]\n",
    "    #     print(f\"###\\n\\nQ: {question}\\nA: {answer}\\n\\n###\")\n",
    "    \n",
    "    # Get graph summary\n",
    "    print(\"\\nKnowledge Graph Summary:\")\n",
    "    print(json.dumps(kg_llm.get_graph_summary(), indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eadcb5ba-a33d-48ce-a81a-b05f40065575",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Knowledge Graph Summary:\n",
      "{\n",
      "  \"node_count\": 10,\n",
      "  \"edge_count\": 0,\n",
      "  \"data\": {\n",
      "    \"people\": {\n",
      "      \"Mrs. Harker\": {},\n",
      "      \"Van Hellingen\": {},\n",
      "      \"seafarers\": {},\n",
      "      \"sailors\": {},\n",
      "      \"Quinci\": {},\n",
      "      \"Harker\": {},\n",
      "      \"She\": {},\n",
      "      \"my friends\": {},\n",
      "      \"he\": {},\n",
      "      \"customs men\": {}\n",
      "    },\n",
      "    \"relations\": {\n",
      "      \"Mrs. Harker\": {},\n",
      "      \"Van Hellingen\": {},\n",
      "      \"seafarers\": {},\n",
      "      \"sailors\": {},\n",
      "      \"Quinci\": {},\n",
      "      \"Harker\": {},\n",
      "      \"She\": {},\n",
      "      \"my friends\": {},\n",
      "      \"he\": {},\n",
      "      \"customs men\": {}\n",
      "    }\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "snippets = [\n",
    "\"\"\"\n",
    "This is written in the train from Varna to Galatz. Last\n",
    "night we all assembled a little before the time of sunset. Each of us\n",
    "had done his work as well as he could; so far as thought, and endeavour,\n",
    "and opportunity go, we are prepared for the whole of our journey, and\n",
    "for our work when we get to Galatz. When the usual time came round Mrs.\n",
    "Harker prepared herself for her hypnotic effort; and after a longer and\n",
    "more serious effort on the part of Van Helsing than has been usually\n",
    "necessary, she sank into the trance. Usually she speaks on a hint; but\n",
    "this time the Professor had to ask her questions, and to ask them pretty\n",
    "resolutely, before we could learn anything; at last her answer came:--\n",
    "\"\"\",\n",
    "\"\"\"\n",
    "“I can see nothing; we are still; there are no waves lapping, but only a\n",
    "steady swirl of water softly running against the hawser. I can hear\n",
    "men’s voices calling, near and far, and the roll and creak of oars in\n",
    "the rowlocks. A gun is fired somewhere; the echo of it seems far away.\n",
    "There is tramping of feet overhead, and ropes and chains are dragged\n",
    "along. What is this? There is a gleam of light; I can feel the air\n",
    "blowing upon me.”\n",
    "\"\"\",\n",
    "\"\"\"\n",
    "Here she stopped. She had risen, as if impulsively, from where she lay\n",
    "on the sofa, and raised both her hands, palms upwards, as if lifting a\n",
    "weight. Van Helsing and I looked at each other with understanding.\n",
    "Quincey raised his eyebrows slightly and looked at her intently, whilst\n",
    "Harker’s hand instinctively closed round the hilt of his Kukri. There\n",
    "was a long pause. We all knew that the time when she could speak was\n",
    "passing; but we felt that it was useless to say anything. Suddenly she\n",
    "sat up, and, as she opened her eyes, said sweetly:--\n",
    "\"\"\",\n",
    "\"\"\"\n",
    "“Would none of you like a cup of tea? You must all be so tired!” We\n",
    "could only make her happy, and so acquiesced. She bustled off to get\n",
    "tea; when she had gone Van Helsing said:--\n",
    "\"\"\",\n",
    "\"\"\"\n",
    "“You see, my friends. _He_ is close to land: he has left his\n",
    "earth-chest. But he has yet to get on shore. In the night he may lie\n",
    "hidden somewhere; but if he be not carried on shore, or if the ship do\n",
    "not touch it, he cannot achieve the land. In such case he can, if it be\n",
    "in the night, change his form and can jump or fly on shore, as he did\n",
    "at Whitby. But if the day come before he get on shore, then, unless he\n",
    "be carried he cannot escape. And if he be carried, then the customs men\n",
    "may discover what the box contain. Thus, in fine, if he escape not on\n",
    "shore to-night, or before dawn, there will be the whole day lost to him.\n",
    "We may then arrive in time; for if he escape not at night we shall come\n",
    "on him in daytime, boxed up and at our mercy; for he dare not be his\n",
    "true self, awake and visible, lest he be discovered.”\n",
    "\"\"\"\n",
    "]\n",
    "questions = []\n",
    "\n",
    "query_llm(KnowledgeGraphLLM(), snippets, \"Dracula\", questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "df76606c-388e-4bbe-aadf-95fd02758ad6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA mem usage: 9.2/12.5GB (73.5%)\n"
     ]
    }
   ],
   "source": [
    "print_mem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "334ed151-b828-4056-a0c1-e5006b92ef31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA mem usage: 9.2/12.5GB (73.5%)\n"
     ]
    }
   ],
   "source": [
    "def parse_book():\n",
    "    # Initialize the system\n",
    "    kg_llm = KnowledgeGraphLLM()\n",
    "    \n",
    "    with open(\"data/dracula.txt\") as f:\n",
    "        context = f.read()\n",
    "        context = context.rpartition(r\"\\n\\nDRACULA\\n\\n\")[-1]\n",
    "\n",
    "    paragraphs = []\n",
    "    chapters = re.split(r\"\\n\\nCHAPTER\\ .*\\n\", context)\n",
    "    for c in chapters:\n",
    "        for p in re.split(r\"\\n\\n\", c):\n",
    "            paragraphs.append(p)\n",
    "\n",
    "\n",
    "    questions = [\n",
    "        \"Who are the main characters in this story?\",\n",
    "        \"What is their relation to each other?\",\n",
    "        \"Who is the antagonist of the story?\",\n",
    "        \"How does the antagonist end up?\"\n",
    "    ]\n",
    "    \n",
    "    query_llm(kg_llm, paragraphs, \"Dracula\", questions)\n",
    "\n",
    "print_mem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d00fce1-9263-4d49-8041-790286da7b74",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
