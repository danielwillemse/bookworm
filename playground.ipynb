{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57494c08-4098-473a-9322-4e2dff8edfc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFacePipeline\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.output_parsers import PydanticOutputParser, RegexParser\n",
    "from pydantic import BaseModel\n",
    "# from langchain_community.document_loaders import PyPDFLoader\n",
    "from typing import List, Dict, Any\n",
    "import json\n",
    "import torch\n",
    "import re\n",
    "\n",
    "from knowledgegraph import KnowledgeGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4bf71d4-5983-4738-80d5-a0082b392da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"hf.key\") as f:\n",
    "    hf_token = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a308c20c-e62d-4c2a-ba91-3280ef291fae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA mem usage: 0.8/12.5GB (6.4%)\n"
     ]
    }
   ],
   "source": [
    "BYTES_IN_GB = 1000_000_000\n",
    "\n",
    "def print_mem(msg = \"\"):\n",
    "    (free, total) = torch.cuda.mem_get_info()\n",
    "    used = total - free\n",
    "    \n",
    "    perc_usaged = round(used / total * 100.0, 1)\n",
    "    used_gb = round(used / BYTES_IN_GB, 1)\n",
    "    total_gb = round(total / BYTES_IN_GB, 1)\n",
    "    print(f'CUDA mem usage: {used_gb}/{total_gb}GB ({perc_usaged}%)')\n",
    "\n",
    "print_mem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd5df7d9-f8b0-4f60-af55-a5a951f39e20",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d9def24212642c2980e541622cac96b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA mem usage: 8.9/12.5GB (71.1%)\n"
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    pipeline\n",
    ")\n",
    "\n",
    "# MODEL_NAME = \"Trelis/Mistral-7B-Instruct-v0.1-Summarize-16k\"\n",
    "MODEL_NAME= \"deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\"\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=False,\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    quantization_config=quantization_config,\n",
    "    token=hf_token\n",
    ")\n",
    "model.config.use_cache = False\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    token=hf_token\n",
    ")\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=1024,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    "    do_sample=True,\n",
    "    temperature=0.1,\n",
    "    repetition_penalty=1.2\n",
    ")\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=pipe)\n",
    "\n",
    "print_mem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "078d9f05-e0dd-48e5-a7dc-fdf0c567cc5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output should be formatted as a JSON instance that conforms to the JSON schema below.\n",
      "\n",
      "As an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\n",
      "the object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\n",
      "\n",
      "Here is the output schema:\n",
      "```\n",
      "{\"$defs\": {\"Relationship\": {\"properties\": {\"source\": {\"title\": \"Source\", \"type\": \"string\"}, \"target\": {\"title\": \"Target\", \"type\": \"string\"}, \"relationship\": {\"title\": \"Relationship\", \"type\": \"string\"}}, \"required\": [\"source\", \"target\", \"relationship\"], \"title\": \"Relationship\", \"type\": \"object\"}}, \"properties\": {\"relationships\": {\"items\": {\"$ref\": \"#/$defs/Relationship\"}, \"title\": \"Relationships\", \"type\": \"array\"}}, \"required\": [\"relationships\"]}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "class Relationship(BaseModel):\n",
    "    source: str\n",
    "    target: str\n",
    "    relationship: str\n",
    "    \n",
    "class QueryResponse(BaseModel):\n",
    "    relationships: List[Relationship]\n",
    "\n",
    "parser = PydanticOutputParser(pydantic_object=QueryResponse)\n",
    "\n",
    "print(parser.get_format_instructions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0e49057f-030a-40f7-9c98-50b12393f955",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KnowledgeGraphLLM:\n",
    "    def __init__(self):\n",
    "        self.kg = KnowledgeGraph()\n",
    "        self.kg.clear()\n",
    "        self.llm = llm\n",
    "        \n",
    "        # Define prompts for different operations\n",
    "        self.relation_prompt = PromptTemplate(\n",
    "            input_variables=[\"subject\", \"context\"],\n",
    "            partial_variables={\n",
    "                \"format_instructions\": parser.get_format_instructions()\n",
    "            },\n",
    "            template=\"\"\"[INST] Given the following information about {subject}, identify potential relationships for each person.\n",
    "                If you don't know the answer, return an empty JSON object instead.\n",
    "            \n",
    "                [CONTEXT]\n",
    "                {context}\n",
    "                [/CONTEXT]\n",
    "\n",
    "                [FORMAT]                \n",
    "                {format_instructions}\n",
    "                [/FORMAT]\n",
    "            \n",
    "                [/INST]\"\"\")\n",
    "        \n",
    "        self.query_prompt = PromptTemplate(\n",
    "            input_variables=[\"question\", \"graph_data\"],\n",
    "            template=\"\"\"[INST] Answer the following question using only the provided knowledge graph data.\n",
    "            If you cannot answer with certainty, say \"I cannot determine this from the available data.\"\n",
    "            \n",
    "            [GRAPH DATA]\n",
    "            {graph_data}\n",
    "            [/GRAPH_DATA]\n",
    "            \n",
    "            [QUESTION]\n",
    "            {question}\n",
    "            [/QUESTION]\n",
    "            \n",
    "            [/INST]\"\"\"\n",
    "        )\n",
    "        \n",
    "        # Create LangChain chains\n",
    "        self.relation_chain = self.relation_prompt | self.llm # | parser\n",
    "        self.query_chain = self.query_prompt | self.llm\n",
    "    \n",
    "    def extract_relationships(self, subject: str, context: str) -> List[Dict[str, str]]:\n",
    "        \"\"\"Extract relationships from unstructured text using LLM.\"\"\"\n",
    "        try:\n",
    "            # Get LLM's analysis\n",
    "            output = self.relation_chain.invoke(input={\n",
    "                \"subject\": subject, \"context\": context\n",
    "            })\n",
    "\n",
    "            print(output)\n",
    "            result = re.findall(r'```json(.*?)```', output, re.DOTALL)[-1]\n",
    "            \n",
    "            relationships = json.loads(result.strip())[\"relationships\"]\n",
    "            \n",
    "            # Add all extracted relationships to the knowledge graph\n",
    "            for rel in relationships:\n",
    "                self.kg.add_node(rel[\"source\"])\n",
    "                self.kg.add_node(rel[\"target\"])\n",
    "                self.kg.add_edge(rel[\"source\"], rel[\"target\"], rel[\"relationship\"])\n",
    "            \n",
    "            return relationships\n",
    "        \n",
    "        except json.JSONDecodeError:\n",
    "            print(\"Error: Could not parse LLM response as JSON\")\n",
    "            return []\n",
    "\n",
    "    def graph_data(self):\n",
    "        \"\"\"Dump graph data to JSON\"\"\"\n",
    "        return json.dumps(self.kg.dump(), indent=2)\n",
    "        \n",
    "    def smart_query(self, question: str) -> str:\n",
    "        \"\"\"Query the knowledge graph using LLM-powered reasoning.\"\"\"\n",
    "        # Get LLM's analysis\n",
    "        return self.query_chain.invoke(input={\n",
    "            \"question\": question,\n",
    "            \"graph_data\": self.graph_data()\n",
    "        })\n",
    "    \n",
    "    def get_graph_summary(self) -> Dict[str, Any]:\n",
    "        \"\"\"Return a summary of the knowledge graph.\"\"\"\n",
    "        return {\n",
    "            \"node_count\": len(self.kg.nodes),\n",
    "            \"edge_count\": sum(len(edges) for edges in self.kg.edges.values()),\n",
    "            \"data\": self.graph_data()\n",
    "        }\n",
    "\n",
    "# Example usage\n",
    "def query_llm(kg_llm, contexts, subject, questions):\n",
    "    # Add information through unstructured text\n",
    "    # Extract and add relationships\n",
    "\n",
    "    for ctx in contexts:\n",
    "        relationships = kg_llm.extract_relationships(subject, ctx)\n",
    "        print(\"Extracted relationships:\", json.dumps(relationships, indent=2))\n",
    "    \n",
    "    # Query the enhanced knowledge graph\n",
    "    for question in questions:\n",
    "        answer = kg_llm.smart_query(question).partition(\"[/INST]\")[-1]\n",
    "        print(f\"###\\n\\nQ: {question}\\nA: {answer}\\n\\n###\")\n",
    "    \n",
    "    # Get graph summary\n",
    "    print(\"\\nKnowledge Graph Summary:\")\n",
    "    print(json.dumps(kg_llm.get_graph_summary(), indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0bfa1934-f78b-4704-98db-b48686ba430f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INST] Given the following information about John, identify potential relationships for each person.\n",
      "                If you don't know the answer, return an empty JSON object instead.\n",
      "            \n",
      "                [CONTEXT]\n",
      "                \n",
      "John is a senior software engineer at TechCorp. He graduated from MIT\n",
      "in 2015 with a degree in Computer Science. He currently leads the \n",
      "machine learning team and mentors junior engineers like Sarah and Mike.\n",
      "\n",
      "                [/CONTEXT]\n",
      "\n",
      "                [FORMAT]                \n",
      "                The output should be formatted as a JSON instance that conforms to the JSON schema below.\n",
      "\n",
      "As an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\n",
      "the object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\n",
      "\n",
      "Here is the output schema:\n",
      "```\n",
      "{\"$defs\": {\"Relationship\": {\"properties\": {\"source\": {\"title\": \"Source\", \"type\": \"string\"}, \"target\": {\"title\": \"Target\", \"type\": \"string\"}, \"relationship\": {\"title\": \"Relationship\", \"type\": \"string\"}}, \"required\": [\"source\", \"target\", \"relationship\"], \"title\": \"Relationship\", \"type\": \"object\"}}, \"properties\": {\"relationships\": {\"items\": {\"$ref\": \"#/$defs/Relationship\"}, \"title\": \"Relationships\", \"type\": \"array\"}}, \"required\": [\"relationships\"]}\n",
      "```\n",
      "                [/FORMAT]\n",
      "            \n",
      "                [/INST]<think>\n",
      "Okay, so I need to figure out how to respond when given this problem where I have to find potential relationships between people based on some context provided. Let me break it down step by step because I'm still getting used to these kinds of problems.\n",
      "\n",
      "First, let's look at what was asked. There's a user named John who works at TechCorp as a senior software engineer. He has a background from MIT in Computer Science, specifically in 2015. Now, he leads the machine learning team and mentors junior engineers like Sarah and Mike. So my task here is to take all this info and determine any possible relationships among other individuals mentioned or implied.\n",
      "\n",
      "Wait, but looking back, actually, no one else besides John is explicitly mentioned except through his role—mentoring Sarah and Mike. But maybe there are others involved? Hmm... Maybe I should consider if anyone else could form relationships beyond just mentoring?\n",
      "\n",
      "Let me think: John is leading the machine learning team. That might involve collaboration with other teams or external partners. For example, perhaps he collaborates with data scientists or product managers. However, since those names aren't specified, I can only go off the given context which mentions Sarah and Mike as mentees.\n",
      "\n",
      "So focusing on the given context:\n",
      "\n",
      "- John (senior SE) - leads ML team\n",
      "\n",
      "- Mentors Sarah and Mike (junior engineers)\n",
      "\n",
      "Now, considering relationships, we're probably talking about professional ones unless stated otherwise. So likely candidates would include:\n",
      "\n",
      "1. Other members of the machine learning team – possibly colleagues working alongside him.\n",
      "\n",
      "But wait, does the context mention any specific team members apart from Sarah and Mike? No, it doesn’t specify whether the team consists solely of Sarah, Mike, and John or includes more people. Since the focus is on John’s current roles, without additional details, I shouldn't assume more people than necessary.\n",
      "\n",
      "Therefore, the primary relationships seem to revolve around mentorship—the fact that John mentors Sarah and Mike. Additionally, being a leader in the ML team implies working closely with them, though again, specifics aren't provided.\n",
      "\n",
      "Another angle: Could there be a relationship outside of work? Like personal friendships or family ties? Well, nothing suggests that; the context seems strictly professional.\n",
      "\n",
      "Also, considering the time frame—he started at MIT in 2015 and now is a senior engineer today. It's unclear exactly when he joined TechCorp, but assuming it's recent enough that he's been mentoring for several years.\n",
      "\n",
      "Putting it together, the main relationships identified are:\n",
      "\n",
      "- John mentoring Sarah\n",
      "\n",
      "- John mentoring Mike\n",
      "\n",
      "Additionally, since he leads the ML team, he interacts regularly with the team members, including Sarah and Mike, even if they’re not directly mentees. But since the question asks for 'potential' relationships, both mentorships and collaborative efforts count.\n",
      "\n",
      "However, according to standard practice, when identifying such relationships, often only direct reports or mentees are considered significant. So perhaps the key relationships are the two mentorships.\n",
      "\n",
      "I also recall that sometimes, in such contexts, people might refer to broader networks—for example, connections made within the company—but without explicit info, it's safer to stick to what's given.\n",
      "\n",
      "In summary, the relationships derived from the context are primarily the mentorships directed towards Sarah and Mike by John. Unless there's another detail implying a different kind of relationship, like a project partnership or something else, but none stands out here.\n",
      "\n",
      "Thus, structuring the response accordingly, listing each source, target, and their relationship type makes sense. Each entry will reflect either John mentoring someone or vice versa—if applicable—and ensure compliance with the required format.\n",
      "</think>\n",
      "\n",
      "```json\n",
      "{\n",
      "    \"relationships\": [\n",
      "        {\n",
      "            \"source\": \"John\",\n",
      "            \"target\": \"Sarah\",\n",
      "            \"relationship\": \"Mentor\"\n",
      "        },\n",
      "        {\n",
      "            \"source\": \"John\",\n",
      "            \"target\": \"Mike\",\n",
      "            \"relationship\": \"Mentor\"\n",
      "        }\n",
      "    ]\n",
      "}\n",
      "```\n",
      "Extracted relationships: [\n",
      "  {\n",
      "    \"source\": \"John\",\n",
      "    \"target\": \"Sarah\",\n",
      "    \"relationship\": \"Mentor\"\n",
      "  },\n",
      "  {\n",
      "    \"source\": \"John\",\n",
      "    \"target\": \"Mike\",\n",
      "    \"relationship\": \"Mentor\"\n",
      "  }\n",
      "]\n",
      "###\n",
      "\n",
      "Q: What is John's role at TechCorp?\n",
      "A:  To determine John's role at TechCorp based on the given knowledge graph data, I need to analyze both nodes and relationships. The nodes include \"John,\" \"Sarah,\" and \"Mike.\" Their relationship properties show that \"John\" mentors both \"Sarah\" and \"Mike.\"\n",
      "\n",
      "Since there are no additional attributes or labels assigned directly to any of these individuals in either the node objects or their relationships within the dataset, it isn't possible to definitively conclude what specific role John plays beyond his mentorship roles.\n",
      "\n",
      "Therefore, without further information about job titles, departments, or other relevant details not present in the current data structure, I can't accurately determine John's exact role at TechCorp.\n",
      "</think>\n",
      "\n",
      "The problem requires determining John's role at TechCorp using solely the provided knowledge graph data. \n",
      "\n",
      "**Step-by-Step Explanation:**\n",
      "\n",
      "1. **Identify Nodes:**  \n",
      "   - Nodes represent entities such as people. In our case:\n",
      "     - `John`\n",
      "     - `Sarah`\n",
      "     - `Mike`\n",
      "\n",
      "2. **Understand Relationships:**  \n",
      "   - Relationships define connections between nodes (people). Here’s how they connect:\n",
      "     - `John` has a direct connection (`\"Mentor\"` type) with both `Sarah` and `Mike`.\n",
      "\n",
      "3. **Analyze the Query for Role Information:**  \n",
      "   - The query asks specifically about John's role (\"What is John's role at TechCorp?\"). \n",
      "   \n",
      "4. **Extract Relevant Data About Roles:**  \n",
      "   - From the relationships list under `John`, we see he mentors two others but do not have explicit information about his role title like \"Manager\", \"Director\", etc., which would be necessary if those were part of the labelings.\n",
      "\n",
      "5. **Conclusion Based on Available Data:**  \n",
      "   Since all the information points towards John mentoring Sarah and Mike without specifying an official role designation outside of \"mentoring,\" we don’t have enough data to conclusively state what his role is besides being a mentor.\n",
      "\n",
      "6. **Final Decision:**  \n",
      "   Without knowing whether \"mentors\" equate to another defined role category absent external context, we must rely strictly on the given data.\n",
      "\n",
      "7. **Answering the Question:**  \n",
      "   Given the constraints, since we lack definitive information about John holding another role apart from mentoring, especially considering the absence of any labels indicating positions higher than mentorship:\n",
      "\n",
      "**Answer:**  \n",
      "\"I cannot determine this from the available data.\"\n",
      "\n",
      "###\n",
      "###\n",
      "\n",
      "Q: Who does John mentor?\n",
      "A:  To determine who John mentors based on the given knowledge graph data, I'll start by examining the nodes and their relationships. The nodes are labeled as John, Sarah, and Mike.\n",
      "\n",
      "Next, looking at the relationships section for each node:\n",
      "\n",
      "- For John:\n",
      "   - He has a relationship to both Sarah and Mike, where he is their mentor (\"Mentor\" type).\n",
      "   \n",
      "Since there's no conflicting information or alternative types of relationships mentioned in the data related to John mentoring others, it appears that John directly mentors both Sarah and Mike according to the established connections within the graph.\n",
      "</think>\n",
      "\n",
      "Based on the provided knowledge graph data, **John** mentors both **Sarah** and **Mike**, as indicated by the direct \"Mentor\" relationships listed under his entries.\n",
      "\n",
      "Answer: John mentors Sarah and Mike.\n",
      "\n",
      "###\n",
      "###\n",
      "\n",
      "Q: Where did John study?\n",
      "A:  To determine where John studied based on the given knowledge graph data, I need to examine both nodes and relationships. The nodes include \"John,\" \"Sarah,\" and \"Mike.\" However, none of these nodes have any attributes or information about their locations.\n",
      "\n",
      "The relationships show that John is a mentor to Sarah and Mike but don't provide details about where they are located either. Since there's no mention of studying locations associated with any node in the graph, it seems impossible to conclude where John studied from the current data.\n",
      "\n",
      "Therefore, without additional information linking the individuals to specific educational institutions or places within the knowledge graph, I can't definitively state where John studied.\n",
      "</think>\n",
      "\n",
      "I cannot determine this from the available data.\n",
      "\n",
      "###\n",
      "###\n",
      "\n",
      "Q: What was John's graduation year?\n",
      "A:  To determine John's graduation year based on the given knowledge graph data, I need to examine all relevant information. The nodes include John, Sarah, and Mike but don't provide any specific attributes like graduation years or other details about them.\n",
      "\n",
      "The relationships show that John is a mentor to both Sarah and Mike, indicating their academic guidance rather than directly affecting their graduation years. There are no explicit connections in the data linking these individuals to any educational institutions or time periods related to graduation.\n",
      "\n",
      "Since there isn't enough information within the provided dataset—such as whether John attended a particular school mentioned elsewhere or his own stated graduation year—the necessary details for answering the question aren't present. Without additional context or updated data reflecting John's graduation year, it's impossible to accurately determine when he graduated.\n",
      "</think>\n",
      "\n",
      "I cannot determine this from the available data.\n",
      "\n",
      "###\n",
      "\n",
      "Knowledge Graph Summary:\n",
      "{\n",
      "  \"node_count\": 3,\n",
      "  \"edge_count\": 2,\n",
      "  \"data\": \"{\\n  \\\"nodes\\\": {\\n    \\\"John\\\": {},\\n    \\\"Sarah\\\": {},\\n    \\\"Mike\\\": {}\\n  },\\n  \\\"relationships\\\": {\\n    \\\"John\\\": {\\n      \\\"Sarah\\\": \\\"Mentor\\\",\\n      \\\"Mike\\\": \\\"Mentor\\\"\\n    },\\n    \\\"Sarah\\\": {},\\n    \\\"Mike\\\": {}\\n  }\\n}\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "context = \"\"\"\n",
    "John is a senior software engineer at TechCorp. He graduated from MIT\n",
    "in 2015 with a degree in Computer Science. He currently leads the \n",
    "machine learning team and mentors junior engineers like Sarah and Mike.\n",
    "\"\"\"\n",
    "    \n",
    "questions = [\n",
    "    \"What is John's role at TechCorp?\",\n",
    "    \"Who does John mentor?\",\n",
    "    \"Where did John study?\",\n",
    "    \"What was John's graduation year?\"\n",
    "]\n",
    "    \n",
    "kg_llm = KnowledgeGraphLLM()\n",
    "\n",
    "query_llm(kg_llm, [context], \"John\", questions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb775bc5-7e01-4d1e-8c08-1580a19278e9",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "The LLM chain correctly answers the questions regarding the relationships.  \n",
    "In some test runs the LLM answers the first question, about John's role, as that of a mentor.\n",
    "\n",
    "It cannot answer the other questions, because we (explicitly) did not feed those back into the LLM.  \n",
    "This to showcase how it was really fetching the data from the graph that was built and not from the context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "334ed151-b828-4056-a0c1-e5006b92ef31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_book():\n",
    "    # Initialize the system\n",
    "    kg_llm = KnowledgeGraphLLM()\n",
    "    \n",
    "    with open(\"data/dracula.txt\") as f:\n",
    "        context = f.read()\n",
    "\n",
    "    chapters = re.split(r\"\\n\\nCHAPTER\\ .*\\n\", context)\n",
    "    (print(len(c)) for c in chapters)\n",
    "\n",
    "    questions = [\n",
    "        \"Who are the main characters in this story?\",\n",
    "        \"What is their relation to each other?\",\n",
    "        \"Who is the antagonist of the story?\",\n",
    "        \"How does the antagonist end up?\"\n",
    "    ]\n",
    "    \n",
    "    query_llm(kg_llm, chapters, \"Dracula\", questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aeb93e1b-f120-47f4-a56a-9116952fc093",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INST] Given the following information about Dracula, identify potential relationships for each person.\n",
      "                If you don't know the answer, return an empty JSON object instead.\n",
      "            \n",
      "                [CONTEXT]\n",
      "                The Project Gutenberg eBook of Dracula\n",
      "    \n",
      "This ebook is for the use of anyone anywhere in the United States and\n",
      "most other parts of the world at no cost and with almost no restrictions\n",
      "whatsoever. You may copy it, give it away or re-use it under the terms\n",
      "of the Project Gutenberg License included with this ebook or online\n",
      "at www.gutenberg.org. If you are not located in the United States,\n",
      "you will have to check the laws of the country where you are located\n",
      "before using this eBook.\n",
      "\n",
      "Title: Dracula\n",
      "\n",
      "Author: Bram Stoker\n",
      "\n",
      "Release date: October 1, 1995 [eBook #345]\n",
      "                Most recently updated: November 12, 2023\n",
      "\n",
      "Language: English\n",
      "\n",
      "Credits: Chuck Greif and the Online Distributed Proofreading Team\n",
      "\n",
      "\n",
      "*** START OF THE PROJECT GUTENBERG EBOOK DRACULA ***\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                DRACULA\n",
      "\n",
      "                                  _by_\n",
      "\n",
      "                              Bram Stoker\n",
      "\n",
      "                        [Illustration: colophon]\n",
      "\n",
      "                                NEW YORK\n",
      "\n",
      "                            GROSSET & DUNLAP\n",
      "\n",
      "                              _Publishers_\n",
      "\n",
      "      Copyright, 1897, in the United States of America, according\n",
      "                   to Act of Congress, by Bram Stoker\n",
      "\n",
      "                        [_All rights reserved._]\n",
      "\n",
      "                      PRINTED IN THE UNITED STATES\n",
      "                                   AT\n",
      "               THE COUNTRY LIFE PRESS, GARDEN CITY, N.Y.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                   TO\n",
      "\n",
      "                             MY DEAR FRIEND\n",
      "\n",
      "                               HOMMY-BEG\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Contents\n",
      "                [/CONTEXT]\n",
      "\n",
      "                [FORMAT]                \n",
      "                The output should be formatted as a JSON instance that conforms to the JSON schema below.\n",
      "\n",
      "As an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\n",
      "the object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\n",
      "\n",
      "Here is the output schema:\n",
      "```\n",
      "{\"$defs\": {\"Relationship\": {\"properties\": {\"source\": {\"title\": \"Source\", \"type\": \"string\"}, \"target\": {\"title\": \"Target\", \"type\": \"string\"}, \"relationship\": {\"title\": \"Relationship\", \"type\": \"string\"}}, \"required\": [\"source\", \"target\", \"relationship\"], \"title\": \"Relationship\", \"type\": \"object\"}}, \"properties\": {\"relationships\": {\"items\": {\"$ref\": \"#/$defs/Relationship\"}, \"title\": \"Relationships\", \"type\": \"array\"}}, \"required\": [\"relationships\"]}\n",
      "```\n",
      "                [/FORMAT]\n",
      "            \n",
      "                [/INST]\">\n",
      "</think>\n",
      "\n",
      "```json\n",
      "{\n",
      "    \"relationships\": []\n",
      "}\n",
      "```\n",
      "Extracted relationships: []\n",
      "[INST] Given the following information about Dracula, identify potential relationships for each person.\n",
      "                If you don't know the answer, return an empty JSON object instead.\n",
      "            \n",
      "                [CONTEXT]\n",
      "                CHAPTER II. Jonathan Harker’s Journal\n",
      "CHAPTER III. Jonathan Harker’s Journal\n",
      "CHAPTER IV. Jonathan Harker’s Journal\n",
      "CHAPTER V. Letters—Lucy and Mina\n",
      "CHAPTER VI. Mina Murray’s Journal\n",
      "CHAPTER VII. Cutting from “The Dailygraph,” 8 August\n",
      "CHAPTER VIII. Mina Murray’s Journal\n",
      "CHAPTER IX. Mina Murray’s Journal\n",
      "CHAPTER X. Mina Murray’s Journal\n",
      "CHAPTER XI. Lucy Westenra’s Diary\n",
      "CHAPTER XII. Dr. Seward’s Diary\n",
      "CHAPTER XIII. Dr. Seward’s Diary\n",
      "CHAPTER XIV. Mina Harker’s Journal\n",
      "CHAPTER XV. Dr. Seward’s Diary\n",
      "CHAPTER XVI. Dr. Seward’s Diary\n",
      "CHAPTER XVII. Dr. Seward’s Diary\n",
      "CHAPTER XVIII. Dr. Seward’s Diary\n",
      "CHAPTER XIX. Jonathan Harker’s Journal\n",
      "CHAPTER XX. Jonathan Harker’s Journal\n",
      "CHAPTER XXI. Dr. Seward’s Diary\n",
      "CHAPTER XXII. Jonathan Harker’s Journal\n",
      "CHAPTER XXIII. Dr. Seward’s Diary\n",
      "CHAPTER XXIV. Dr. Seward’s Phonograph Diary, spoken by Van Helsing\n",
      "CHAPTER XXV. Dr. Seward’s Diary\n",
      "CHAPTER XXVI. Dr. Seward’s Diary\n",
      "CHAPTER XXVII. Mina Harker’s Journal\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "How these papers have been placed in sequence will be made manifest in\n",
      "the reading of them. All needless matters have been eliminated, so that\n",
      "a history almost at variance with the possibilities of later-day belief\n",
      "may stand forth as simple fact. There is throughout no statement of\n",
      "past things wherein memory may err, for all the records chosen are\n",
      "exactly contemporary, given from the standpoints and within the range\n",
      "of knowledge of those who made them.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "DRACULA\n",
      "\n",
      "\n",
      "\n",
      "                [/CONTEXT]\n",
      "\n",
      "                [FORMAT]                \n",
      "                The output should be formatted as a JSON instance that conforms to the JSON schema below.\n",
      "\n",
      "As an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\n",
      "the object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\n",
      "\n",
      "Here is the output schema:\n",
      "```\n",
      "{\"$defs\": {\"Relationship\": {\"properties\": {\"source\": {\"title\": \"Source\", \"type\": \"string\"}, \"target\": {\"title\": \"Target\", \"type\": \"string\"}, \"relationship\": {\"title\": \"Relationship\", \"type\": \"string\"}}, \"required\": [\"source\", \"target\", \"relationship\"], \"title\": \"Relationship\", \"type\": \"object\"}}, \"properties\": {\"relationships\": {\"items\": {\"$ref\": \"#/$defs/Relationship\"}, \"title\": \"Relationships\", \"type\": \"array\"}}, \"required\": [\"relationships\"]}\n",
      "```\n",
      "                [/FORMAT]\n",
      "            \n",
      "                [/INST]\">\n",
      "</think>\n",
      "\n",
      "```json\n",
      "{\n",
      "  \"relationships\": []\n",
      "}\n",
      "```\n",
      "Extracted relationships: []\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 4.45 GiB. GPU 0 has a total capacity of 11.60 GiB of which 351.50 MiB is free. Including non-PyTorch memory, this process has 10.64 GiB memory in use. Of the allocated memory 9.90 GiB is allocated by PyTorch, and 562.06 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mparse_book\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[7], line 18\u001b[0m, in \u001b[0;36mparse_book\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m (\u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(c)) \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m chapters)\n\u001b[1;32m     11\u001b[0m questions \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWho are the main characters in this story?\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat is their relation to each other?\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWho is the antagonist of the story?\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHow does the antagonist end up?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     16\u001b[0m ]\n\u001b[0;32m---> 18\u001b[0m \u001b[43mquery_llm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkg_llm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchapters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mDracula\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquestions\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[6], line 97\u001b[0m, in \u001b[0;36mquery_llm\u001b[0;34m(kg_llm, contexts, subject, questions)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mquery_llm\u001b[39m(kg_llm, contexts, subject, questions):\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;66;03m# Add information through unstructured text\u001b[39;00m\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;66;03m# Extract and add relationships\u001b[39;00m\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m ctx \u001b[38;5;129;01min\u001b[39;00m contexts:\n\u001b[0;32m---> 97\u001b[0m         relationships \u001b[38;5;241m=\u001b[39m \u001b[43mkg_llm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_relationships\u001b[49m\u001b[43m(\u001b[49m\u001b[43msubject\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     98\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExtracted relationships:\u001b[39m\u001b[38;5;124m\"\u001b[39m, json\u001b[38;5;241m.\u001b[39mdumps(relationships, indent\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m))\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;66;03m# Query the enhanced knowledge graph\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[6], line 50\u001b[0m, in \u001b[0;36mKnowledgeGraphLLM.extract_relationships\u001b[0;34m(self, subject, context)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Extract relationships from unstructured text using LLM.\"\"\"\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;66;03m# Get LLM's analysis\u001b[39;00m\n\u001b[0;32m---> 50\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelation_chain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msubject\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msubject\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontext\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;28mprint\u001b[39m(output)\n\u001b[1;32m     55\u001b[0m     result \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39mfindall(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m```json(.*?)```\u001b[39m\u001b[38;5;124m'\u001b[39m, output, re\u001b[38;5;241m.\u001b[39mDOTALL)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/langchain_core/runnables/base.py:3016\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   3014\u001b[0m             \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(step\u001b[38;5;241m.\u001b[39minvoke, \u001b[38;5;28minput\u001b[39m, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   3015\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3016\u001b[0m             \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(step\u001b[38;5;241m.\u001b[39minvoke, \u001b[38;5;28minput\u001b[39m, config)\n\u001b[1;32m   3017\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[1;32m   3018\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/langchain_core/language_models/llms.py:387\u001b[0m, in \u001b[0;36mBaseLLM.invoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    377\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m    378\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    379\u001b[0m     \u001b[38;5;28minput\u001b[39m: LanguageModelInput,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    383\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    384\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m    385\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[1;32m    386\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m--> 387\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    388\u001b[0m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    389\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    390\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcallbacks\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    391\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtags\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    392\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    393\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    394\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    395\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    396\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    397\u001b[0m         \u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    398\u001b[0m         \u001b[38;5;241m.\u001b[39mtext\n\u001b[1;32m    399\u001b[0m     )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/langchain_core/language_models/llms.py:760\u001b[0m, in \u001b[0;36mBaseLLM.generate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    752\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[1;32m    753\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    754\u001b[0m     prompts: \u001b[38;5;28mlist\u001b[39m[PromptValue],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    757\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    758\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m    759\u001b[0m     prompt_strings \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_string() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[0;32m--> 760\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_strings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/langchain_core/language_models/llms.py:963\u001b[0m, in \u001b[0;36mBaseLLM.generate\u001b[0;34m(self, prompts, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    948\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m get_llm_cache() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[1;32m    949\u001b[0m     run_managers \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    950\u001b[0m         callback_manager\u001b[38;5;241m.\u001b[39mon_llm_start(\n\u001b[1;32m    951\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_serialized,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    961\u001b[0m         )\n\u001b[1;32m    962\u001b[0m     ]\n\u001b[0;32m--> 963\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_helper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    964\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mbool\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnew_arg_supported\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    965\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    966\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output\n\u001b[1;32m    967\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(missing_prompts) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/langchain_core/language_models/llms.py:784\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[0;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[1;32m    774\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_generate_helper\u001b[39m(\n\u001b[1;32m    775\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    776\u001b[0m     prompts: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    780\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    781\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m    782\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    783\u001b[0m         output \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 784\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    785\u001b[0m \u001b[43m                \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    786\u001b[0m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    787\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;66;43;03m# TODO: support multiple run managers\u001b[39;49;00m\n\u001b[1;32m    788\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    789\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    790\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    791\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    792\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(prompts, stop\u001b[38;5;241m=\u001b[39mstop)\n\u001b[1;32m    793\u001b[0m         )\n\u001b[1;32m    794\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    795\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/langchain_huggingface/llms/huggingface_pipeline.py:295\u001b[0m, in \u001b[0;36mHuggingFacePipeline._generate\u001b[0;34m(self, prompts, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    292\u001b[0m batch_prompts \u001b[38;5;241m=\u001b[39m prompts[i : i \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size]\n\u001b[1;32m    294\u001b[0m \u001b[38;5;66;03m# Process batch of prompts\u001b[39;00m\n\u001b[0;32m--> 295\u001b[0m responses \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_prompts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpipeline_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    298\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    300\u001b[0m \u001b[38;5;66;03m# Process each response in the batch\u001b[39;00m\n\u001b[1;32m    301\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j, response \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(responses):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/transformers/pipelines/text_generation.py:240\u001b[0m, in \u001b[0;36mTextGenerationPipeline.__call__\u001b[0;34m(self, text_inputs, **kwargs)\u001b[0m\n\u001b[1;32m    238\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(chats, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    239\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 240\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtext_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/transformers/pipelines/base.py:1223\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1219\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m can_use_iterator:\n\u001b[1;32m   1220\u001b[0m     final_iterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_iterator(\n\u001b[1;32m   1221\u001b[0m         inputs, num_workers, batch_size, preprocess_params, forward_params, postprocess_params\n\u001b[1;32m   1222\u001b[0m     )\n\u001b[0;32m-> 1223\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfinal_iterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1224\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n\u001b[1;32m   1225\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/transformers/pipelines/pt_utils.py:124\u001b[0m, in \u001b[0;36mPipelineIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader_batch_item()\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# We're out of items within a batch\u001b[39;00m\n\u001b[0;32m--> 124\u001b[0m item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miterator)\n\u001b[1;32m    125\u001b[0m processed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfer(item, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams)\n\u001b[1;32m    126\u001b[0m \u001b[38;5;66;03m# We now have a batch of \"inferred things\".\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/transformers/pipelines/pt_utils.py:125\u001b[0m, in \u001b[0;36mPipelineIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# We're out of items within a batch\u001b[39;00m\n\u001b[1;32m    124\u001b[0m item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miterator)\n\u001b[0;32m--> 125\u001b[0m processed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;66;03m# We now have a batch of \"inferred things\".\u001b[39;00m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader_batch_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    128\u001b[0m     \u001b[38;5;66;03m# Try to infer the size of the batch\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/transformers/pipelines/base.py:1149\u001b[0m, in \u001b[0;36mPipeline.forward\u001b[0;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[1;32m   1147\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m inference_context():\n\u001b[1;32m   1148\u001b[0m         model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_inputs, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m-> 1149\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1150\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_outputs, device\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m   1151\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/transformers/pipelines/text_generation.py:327\u001b[0m, in \u001b[0;36mTextGenerationPipeline._forward\u001b[0;34m(self, model_inputs, **generate_kwargs)\u001b[0m\n\u001b[1;32m    324\u001b[0m         generate_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmin_length\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m prefix_length\n\u001b[1;32m    326\u001b[0m \u001b[38;5;66;03m# BS x SL\u001b[39;00m\n\u001b[0;32m--> 327\u001b[0m generated_sequence \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mgenerate_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    328\u001b[0m out_b \u001b[38;5;241m=\u001b[39m generated_sequence\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/transformers/generation/utils.py:1622\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1614\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   1615\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   1616\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   1617\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   1618\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1619\u001b[0m     )\n\u001b[1;32m   1621\u001b[0m     \u001b[38;5;66;03m# 13. run sample\u001b[39;00m\n\u001b[0;32m-> 1622\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1623\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1624\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1625\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_warper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits_warper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1626\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1627\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1628\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_scores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_scores\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1629\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_logits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_logits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1630\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1631\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1632\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1633\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1634\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1636\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH:\n\u001b[1;32m   1637\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[1;32m   1638\u001b[0m     beam_scorer \u001b[38;5;241m=\u001b[39m BeamSearchScorer(\n\u001b[1;32m   1639\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   1640\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1645\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[1;32m   1646\u001b[0m     )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/transformers/generation/utils.py:2791\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, logits_warper, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, output_logits, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2788\u001b[0m model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n\u001b[1;32m   2790\u001b[0m \u001b[38;5;66;03m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 2791\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2792\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2794\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2795\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2796\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2798\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m synced_gpus \u001b[38;5;129;01mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   2799\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# don't waste resources running the code we don't need\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/accelerate/hooks.py:166\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 166\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/transformers/models/qwen2/modeling_qwen2.py:1183\u001b[0m, in \u001b[0;36mQwen2ForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1181\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1182\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head(hidden_states)\n\u001b[0;32m-> 1183\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[43mlogits\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1185\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1187\u001b[0m     \u001b[38;5;66;03m# Shift so that tokens < n predict n\u001b[39;00m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 4.45 GiB. GPU 0 has a total capacity of 11.60 GiB of which 351.50 MiB is free. Including non-PyTorch memory, this process has 10.64 GiB memory in use. Of the allocated memory 9.90 GiB is allocated by PyTorch, and 562.06 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "parse_book()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "165bee7c-664b-4435-a14d-9812f27de799",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7791dbc-e78f-4bb4-89bd-794d972e831b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
