{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "57494c08-4098-473a-9322-4e2dff8edfc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFacePipeline\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.output_parsers import PydanticOutputParser, RegexParser\n",
    "from pydantic import BaseModel\n",
    "from typing import List, Dict, Any\n",
    "import json\n",
    "import torch\n",
    "import re\n",
    "\n",
    "from knowledgegraph import KnowledgeGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4bf71d4-5983-4738-80d5-a0082b392da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"hf.key\") as f:\n",
    "    hf_token = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a308c20c-e62d-4c2a-ba91-3280ef291fae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA mem usage: 0.8/12.5GB (6.7%)\n"
     ]
    }
   ],
   "source": [
    "BYTES_IN_GB = 1000_000_000\n",
    "\n",
    "def print_mem(msg = \"\"):\n",
    "    (free, total) = torch.cuda.mem_get_info()\n",
    "    used = total - free\n",
    "    \n",
    "    perc_usaged = round(used / total * 100.0, 1)\n",
    "    used_gb = round(used / BYTES_IN_GB, 1)\n",
    "    total_gb = round(total / BYTES_IN_GB, 1)\n",
    "    print(f'CUDA mem usage: {used_gb}/{total_gb}GB ({perc_usaged}%)')\n",
    "\n",
    "print_mem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd5df7d9-f8b0-4f60-af55-a5a951f39e20",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e4e50528cd54e7fbf2997eaaea3ca06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA mem usage: 8.8/12.5GB (71.0%)\n"
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    pipeline\n",
    ")\n",
    "\n",
    "# MODEL_NAME = \"Trelis/Mistral-7B-Instruct-v0.1-Summarize-16k\"\n",
    "MODEL_NAME= \"deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\"\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=False,\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    quantization_config=quantization_config,\n",
    "    token=hf_token\n",
    ")\n",
    "model.config.use_cache = False\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    token=hf_token\n",
    ")\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=512,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    "    do_sample=True,\n",
    "    temperature=0.1,\n",
    "    repetition_penalty=1.2\n",
    ")\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=pipe)\n",
    "\n",
    "print_mem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "078d9f05-e0dd-48e5-a7dc-fdf0c567cc5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output should be formatted as a JSON instance that conforms to the JSON schema below.\n",
      "\n",
      "As an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\n",
      "the object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\n",
      "\n",
      "Here is the output schema:\n",
      "```\n",
      "{\"$defs\": {\"Person\": {\"properties\": {\"name\": {\"title\": \"Name\", \"type\": \"string\"}, \"role\": {\"title\": \"Role\", \"type\": \"string\"}}, \"required\": [\"name\", \"role\"], \"title\": \"Person\", \"type\": \"object\"}, \"Relationship\": {\"properties\": {\"source\": {\"$ref\": \"#/$defs/Person\"}, \"target\": {\"$ref\": \"#/$defs/Person\"}, \"relationship\": {\"title\": \"Relationship\", \"type\": \"string\"}}, \"required\": [\"source\", \"target\", \"relationship\"], \"title\": \"Relationship\", \"type\": \"object\"}}, \"properties\": {\"relationships\": {\"items\": {\"$ref\": \"#/$defs/Relationship\"}, \"title\": \"Relationships\", \"type\": \"array\"}}, \"required\": [\"relationships\"]}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "class Person(BaseModel):\n",
    "    name: str\n",
    "    role: str\n",
    "\n",
    "class Relationship(BaseModel):\n",
    "    source: Person\n",
    "    target: Person\n",
    "    relationship: str\n",
    "    \n",
    "class QueryResponse(BaseModel):\n",
    "    relationships: List[Relationship]\n",
    "\n",
    "parser = PydanticOutputParser(pydantic_object=QueryResponse)\n",
    "\n",
    "print(parser.get_format_instructions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0e49057f-030a-40f7-9c98-50b12393f955",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KnowledgeGraphLLM:\n",
    "    def __init__(self):\n",
    "        self.kg = KnowledgeGraph()\n",
    "        self.llm = llm\n",
    "        \n",
    "        # Define prompts for different operations\n",
    "        self.relation_prompt = PromptTemplate(\n",
    "            input_variables=[\"subject\", \"context\"],\n",
    "            partial_variables={\n",
    "                \"format_instructions\": parser.get_format_instructions()\n",
    "            },\n",
    "            template=\"\"\"[INST] Given the following information about {subject}, identify potential relationships for each person.\n",
    "                If you don't know the answer, return an empty JSON object instead.\n",
    "            \n",
    "                [CONTEXT]\n",
    "                {context}\n",
    "                [/CONTEXT]\n",
    "\n",
    "                [FORMAT]                \n",
    "                {format_instructions}\n",
    "                [/FORMAT]\n",
    "            \n",
    "                [/INST]\"\"\")\n",
    "        \n",
    "        self.query_prompt = PromptTemplate(\n",
    "            input_variables=[\"question\", \"graph_data\"],\n",
    "            template=\"\"\"[INST] Answer the following question using only the provided knowledge graph data.\n",
    "            If you cannot answer with certainty, say \"I cannot determine this from the available data.\"\n",
    "            \n",
    "            [GRAPH DATA]\n",
    "            {graph_data}\n",
    "            [/GRAPH_DATA]\n",
    "            \n",
    "            [QUESTION]\n",
    "            {question}\n",
    "            [/QUESTION]\n",
    "            \n",
    "            [/INST]\"\"\"\n",
    "        )\n",
    "        \n",
    "        # Create LangChain chains\n",
    "        self.relation_chain = self.relation_prompt | self.llm # | parser\n",
    "        self.query_chain = self.query_prompt | self.llm\n",
    "    \n",
    "    def extract_relationships(self, subject: str, context: str) -> List[Dict[str, str]]:\n",
    "        \"\"\"Extract relationships from unstructured text using LLM.\"\"\"\n",
    "        try:\n",
    "            # Get LLM's analysis\n",
    "            output = self.relation_chain.invoke(input={\n",
    "                \"subject\": subject, \"context\": context\n",
    "            })\n",
    "\n",
    "            result = re.findall(r'```json(.*?)```', string, re.DOTALL)[-1]\n",
    "            \n",
    "            relationships = json.loads(result.strip())[\"relationships\"]\n",
    "            \n",
    "            # Add all extracted relationships to the knowledge graph\n",
    "            for rel in relationships:\n",
    "                self.kg.add_node(rel[\"source\"])\n",
    "                self.kg.add_node(rel[\"target\"])\n",
    "                self.kg.add_edge(rel[\"source\"], rel[\"target\"], rel[\"relationship\"])\n",
    "            \n",
    "            return relationships\n",
    "        \n",
    "        except json.JSONDecodeError:\n",
    "            print(\"Error: Could not parse LLM response as JSON\")\n",
    "            return []\n",
    "\n",
    "    def graph_data(self):\n",
    "        \"\"\"Dump graph data to JSON\"\"\"\n",
    "        return json.dumps(self.kg.dump(), indent=2)\n",
    "        \n",
    "    def smart_query(self, question: str) -> str:\n",
    "        \"\"\"Query the knowledge graph using LLM-powered reasoning.\"\"\"\n",
    "        # Get LLM's analysis\n",
    "        return self.query_chain.invoke(input={\n",
    "            \"question\": question,\n",
    "            \"graph_data\": self.graph_data()\n",
    "        })\n",
    "    \n",
    "    def get_graph_summary(self) -> Dict[str, Any]:\n",
    "        \"\"\"Return a summary of the knowledge graph.\"\"\"\n",
    "        return {\n",
    "            \"node_count\": len(self.kg.nodes),\n",
    "            \"edge_count\": sum(len(edges) for edges in self.kg.edges.values()),\n",
    "            \"data\": self.graph_data()\n",
    "        }\n",
    "\n",
    "# Example usage\n",
    "def example_usage():\n",
    "    # Initialize the system\n",
    "    kg_llm = KnowledgeGraphLLM()\n",
    "    \n",
    "    # Add information through unstructured text\n",
    "    context = \"\"\"\n",
    "    John is a senior software engineer at TechCorp. He graduated from MIT\n",
    "    in 2015 with a degree in Computer Science. He currently leads the \n",
    "    machine learning team and mentors junior engineers like Sarah and Mike.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Extract and add relationships\n",
    "    relationships = kg_llm.extract_relationships(\"John\", context)\n",
    "    print(\"Extracted relationships:\", json.dumps(relationships, indent=2))\n",
    "    \n",
    "    # Query the enhanced knowledge graph\n",
    "    questions = [\n",
    "        \"What is John's role at TechCorp?\",\n",
    "        \"Who does John mentor?\",\n",
    "        \"Where did John study?\",\n",
    "        \"What was John's graduation year?\"\n",
    "    ]\n",
    "    \n",
    "    for question in questions:\n",
    "        answer = kg_llm.smart_query(question).partition(\"[/INST]\")[-1]\n",
    "        print(f\"###\\n\\nQ: {question}\\nA: {answer}\\n\\n###\")\n",
    "    \n",
    "    # Get graph summary\n",
    "    print(\"\\nKnowledge Graph Summary:\")\n",
    "    print(json.dumps(kg_llm.get_graph_summary(), indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0bfa1934-f78b-4704-98db-b48686ba430f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted relationships: [\n",
      "  {\n",
      "    \"source\": \"John\",\n",
      "    \"target\": \"Sarah\",\n",
      "    \"relationship\": \"Mentorship\"\n",
      "  },\n",
      "  {\n",
      "    \"source\": \"John\",\n",
      "    \"target\": \"Mike\",\n",
      "    \"relationship\": \"Mentorship\"\n",
      "  }\n",
      "]\n",
      "###\n",
      "\n",
      "Q: What is John's role at TechCorp?\n",
      "A:  To determine John's role at TechCorp based on the given knowledge graph data, I first examine the nodes and relationships. The nodes include \"John,\" \"Sarah,\" and \"Mike.\" There are no specific attributes assigned to any of these individuals within the relationship definitions.\n",
      "\n",
      "The relationships show that John has a mentorship link with both Sarah and Mike but do not provide information about their roles or positions within the company like TechCorp. Without additional details such as job titles mentioned in the relationships or explicit node descriptions, it's impossible to ascertain what role John plays specifically at TechCorp.\n",
      "\n",
      "Therefore, due to insufficient data indicating John's position or responsibilities at TechCorp beyond being a mentee, I conclude that I cannot determine his exact role confidently from the current information.\n",
      "</think>\n",
      "\n",
      "I cannot determine this from the available data.\n",
      "\n",
      "###\n",
      "###\n",
      "\n",
      "Q: Who does John mentor?\n",
      "A:  To determine who John mentors based on the given knowledge graph data, I need to examine both nodes and relationships. The nodes include three individuals: John, Sarah, and Mike, each without any attributes listed.\n",
      "\n",
      "The relationships section shows that John has two connections:\n",
      "- One relationship is between John and Sarah labeled as \"Mentorship\".\n",
      "- Another relationship exists between John and Mike also marked as \"Mentorship\".\n",
      "\n",
      "Since there are no conflicting or additional information about these relationships within the dataset, it's clear that John mentors both Sarah and Mike individually through separate instances of Mentorship.\n",
      "\n",
      "Therefore, according to the provided data, John mentors Sarah directly via a single instance of Mentorship and mentors Mike separately through another instance of the same type of relationship.\n",
      "</think>\n",
      "\n",
      "Based on the provided knowledge graph:\n",
      "\n",
      "1. **Identify Relationships**: \n",
      "   - John has two relationships in his list under \"Mentorship\":\n",
      "     - With Sarah\n",
      "     - With Mike\n",
      "\n",
      "2. **Conclusion**:\n",
      "   - Therefore, John mentors both Sarah and Mike.\n",
      "\n",
      "Answer: John mentors Sarah and Mike.\n",
      "\n",
      "###\n",
      "###\n",
      "\n",
      "Q: Where did John study?\n",
      "A:  To determine where John studied based on the given knowledge graph data, I need to examine both nodes and relationships. The nodes include \"John,\" \"Sarah,\" and \"Mike.\" However, none of these nodes have any attributes or information about their locations—such as a city or country they might be studying in.\n",
      "\n",
      "The relationships between them are all types of mentorships: John is mentoring Sarah and Mike. Since there's no mention of educational institutions or specific places associated with either person, it appears that we don't have enough data to pinpoint where John studied.\n",
      "\n",
      "Therefore, without additional information linking the individuals to specific geographic locations within the dataset, I can't definitively answer where John studied.\n",
      "</think>\n",
      "\n",
      "I cannot determine this from the available data.\n",
      "\n",
      "###\n",
      "###\n",
      "\n",
      "Q: What was John's graduation year?\n",
      "A:  To determine John's graduation year based on the given knowledge graph data, I need to examine all relevant information. The nodes include John, Sarah, and Mike but don't provide any specific attributes like their years of graduation.\n",
      "\n",
      "The relationships show that John is a mentor to both Sarah and Mike through \"Mentorship\" links. However, there’s no indication in either the node or relationship details about academic achievements such as graduation years.\n",
      "\n",
      "Without explicit data linking individuals to their graduation years within the knowledge graph, it's impossible to ascertain John's specific graduation year. Therefore, I must conclude that insufficient information exists to make an accurate determination.\n",
      "</think>\n",
      "\n",
      "I cannot determine this from the available data.\n",
      "\n",
      "###\n",
      "\n",
      "Knowledge Graph Summary:\n",
      "{\n",
      "  \"node_count\": 3,\n",
      "  \"edge_count\": 2,\n",
      "  \"data\": \"{\\n  \\\"nodes\\\": {\\n    \\\"John\\\": {},\\n    \\\"Sarah\\\": {},\\n    \\\"Mike\\\": {}\\n  },\\n  \\\"relationships\\\": {\\n    \\\"John\\\": {\\n      \\\"Sarah\\\": \\\"Mentorship\\\",\\n      \\\"Mike\\\": \\\"Mentorship\\\"\\n    },\\n    \\\"Sarah\\\": {},\\n    \\\"Mike\\\": {}\\n  }\\n}\"\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.KnowledgeGraphLLM at 0x7d2e2d9c8f10>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_usage()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb775bc5-7e01-4d1e-8c08-1580a19278e9",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "The LLM chain correctly answers the questions regarding the relationships.  \n",
    "In some test runs the LLM answers the first question, about John's role, as that of a mentor.\n",
    "\n",
    "It cannot answer the other questions, because we (explicitly) did not feed those back into the LLM.  \n",
    "This to showcase how it was really fetching the data from the graph that was built and not from the context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb547611-e47d-4b50-8918-f7ae56008703",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
