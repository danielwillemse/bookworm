{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57494c08-4098-473a-9322-4e2dff8edfc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFacePipeline\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.output_parsers import PydanticOutputParser, RegexParser\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Dict, Any\n",
    "import json\n",
    "import torch\n",
    "import re\n",
    "import time\n",
    "\n",
    "from knowledgegraph import KnowledgeGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc877111-2b58-4f10-87e9-8ff0d31e2dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"hf.key\") as f:\n",
    "    hf_token = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a308c20c-e62d-4c2a-ba91-3280ef291fae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA mem usage: 2.6/12.5GB (21.0%)\n"
     ]
    }
   ],
   "source": [
    "BYTES_IN_GB = 1000_000_000\n",
    "\n",
    "def print_mem(msg = \"\"):\n",
    "    (free, total) = torch.cuda.mem_get_info()\n",
    "    used = total - free\n",
    "    \n",
    "    perc_usaged = round(used / total * 100.0, 1)\n",
    "    used_gb = round(used / BYTES_IN_GB, 1)\n",
    "    total_gb = round(total / BYTES_IN_GB, 1)\n",
    "    print(f'CUDA mem usage: {used_gb}/{total_gb}GB ({perc_usaged}%)')\n",
    "\n",
    "print_mem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd5df7d9-f8b0-4f60-af55-a5a951f39e20",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5548ffd00e740778abbb244ed244277",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    pipeline\n",
    ")\n",
    "\n",
    "# MODEL_NAME = \"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\"\n",
    "MODEL_NAME= \"deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\"\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=False,\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    quantization_config=quantization_config,\n",
    "    token=hf_token\n",
    ")\n",
    "model.config.use_cache = False\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    token=hf_token\n",
    ")\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=2048,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    "    repetition_penalty=1.2\n",
    ")\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a994ea41-e65c-421c-9080-91c0e38ec59c",
   "metadata": {},
   "outputs": [],
   "source": [
    "context = \"\"\"\n",
    "This is written in the train from Varna to Galatz. Last\n",
    "night we all assembled a little before the time of sunset. Each of us\n",
    "had done his work as well as he could; so far as thought, and endeavour,\n",
    "and opportunity go, we are prepared for the whole of our journey, and\n",
    "for our work when we get to Galatz. When the usual time came round Mrs.\n",
    "Harker prepared herself for her hypnotic effort; and after a longer and\n",
    "more serious effort on the part of Van Helsing than has been usually\n",
    "necessary, she sank into the trance. Usually she speaks on a hint; but\n",
    "this time the Professor had to ask her questions, and to ask them pretty\n",
    "resolutely, before we could learn anything; at last her answer came:--\n",
    "“I can see nothing; we are still; there are no waves lapping, but only a\n",
    "steady swirl of water softly running against the hawser. I can hear\n",
    "men’s voices calling, near and far, and the roll and creak of oars in\n",
    "the rowlocks. A gun is fired somewhere; the echo of it seems far away.\n",
    "There is tramping of feet overhead, and ropes and chains are dragged\n",
    "along. What is this? There is a gleam of light; I can feel the air\n",
    "blowing upon me.”\n",
    "Here she stopped. She had risen, as if impulsively, from where she lay\n",
    "on the sofa, and raised both her hands, palms upwards, as if lifting a\n",
    "weight. Van Helsing and I looked at each other with understanding.\n",
    "Quincey raised his eyebrows slightly and looked at her intently, whilst\n",
    "Harker’s hand instinctively closed round the hilt of his Kukri. There\n",
    "was a long pause. We all knew that the time when she could speak was\n",
    "passing; but we felt that it was useless to say anything. Suddenly she\n",
    "sat up, and, as she opened her eyes, said sweetly:--\n",
    "“Would none of you like a cup of tea? You must all be so tired!” We\n",
    "could only make her happy, and so acquiesced. She bustled off to get\n",
    "tea; when she had gone Van Helsing said:--\n",
    "“You see, my friends. _He_ is close to land: he has left his\n",
    "earth-chest. But he has yet to get on shore. In the night he may lie\n",
    "hidden somewhere; but if he be not carried on shore, or if the ship do\n",
    "not touch it, he cannot achieve the land. In such case he can, if it be\n",
    "in the night, change his form and can jump or fly on shore, as he did\n",
    "at Whitby. But if the day come before he get on shore, then, unless he\n",
    "be carried he cannot escape. And if he be carried, then the customs men\n",
    "may discover what the box contain. Thus, in fine, if he escape not on\n",
    "shore to-night, or before dawn, there will be the whole day lost to him.\n",
    "We may then arrive in time; for if he escape not at night we shall come\n",
    "on him in daytime, boxed up and at our mercy; for he dare not be his\n",
    "true self, awake and visible, lest he be discovered.”\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "078d9f05-e0dd-48e5-a7dc-fdf0c567cc5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output should be formatted as a JSON instance that conforms to the JSON schema below.\n",
      "\n",
      "As an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\n",
      "the object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\n",
      "\n",
      "Here is the output schema:\n",
      "```\n",
      "{\"$defs\": {\"Relation\": {\"properties\": {\"source\": {\"description\": \"Name\", \"title\": \"Source\", \"type\": \"string\"}, \"target\": {\"description\": \"Name\", \"title\": \"Target\", \"type\": \"string\"}, \"type\": {\"description\": \"Type of relation. E.g.: Friend, colleague, relative\", \"title\": \"Type\", \"type\": \"string\"}}, \"required\": [\"source\", \"target\", \"type\"], \"title\": \"Relation\", \"type\": \"object\"}}, \"properties\": {\"relations\": {\"items\": {\"$ref\": \"#/$defs/Relation\"}, \"title\": \"Relations\", \"type\": \"array\"}}, \"required\": [\"relations\"]}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "class Relation(BaseModel):\n",
    "    source: str = Field(description=\"Name\")\n",
    "    target: str = Field(description=\"Name\")\n",
    "    type: str = Field(description=\"Type of relation. E.g.: Friend, colleague, relative\")\n",
    "    \n",
    "class Relations(BaseModel):\n",
    "    relations: List[Relation]\n",
    "\n",
    "rel_parser = PydanticOutputParser(pydantic_object=Relations)\n",
    "\n",
    "print(rel_parser.get_format_instructions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7388d2e7-c987-49d1-9aca-9222ed7e4feb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['context'] input_types={} partial_variables={'format_instructions': 'The output should be formatted as a JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output schema:\\n```\\n{\"$defs\": {\"Relation\": {\"properties\": {\"source\": {\"description\": \"Name\", \"title\": \"Source\", \"type\": \"string\"}, \"target\": {\"description\": \"Name\", \"title\": \"Target\", \"type\": \"string\"}, \"type\": {\"description\": \"Type of relation. E.g.: Friend, colleague, relative\", \"title\": \"Type\", \"type\": \"string\"}}, \"required\": [\"source\", \"target\", \"type\"], \"title\": \"Relation\", \"type\": \"object\"}}, \"properties\": {\"relations\": {\"items\": {\"$ref\": \"#/$defs/Relation\"}, \"title\": \"Relations\", \"type\": \"array\"}}, \"required\": [\"relations\"]}\\n```'} template='<｜begin▁of▁sentence｜><｜Assistant｜>You are an expert on reading novels. Read the given context and answer the question of the user.<｜end▁of▁sentence｜><｜User｜>Find all people and their relationship to each other.\\n\\n{format_instructions}\\n\\n{context}\\n\\n'\n",
      "<think>\n",
      "Alright, so I need to figure out how to extract all the people and their relationships from this passage. Let me read through it carefully.\n",
      "\n",
      "First, I'll identify all the characters mentioned. The passage is about a train journey, so the main people are the narrator (probably Mr. Quincey), Van Helsing, Mrs. Harker, Quincey, and the Professor. Wait, the passage starts with \"The train from Varna to Galatz,\" but it doesn't explicitly say who is on the train. So, the people on the train would be Van Helsing, Mrs. Harker, Quincey, and the Professor. \n",
      "\n",
      "Now, I need to see how they relate to each other. The Professor is a passenger, as are Van Helsing and Mrs. Harker. Quincey is also a passenger. So, the relationships here are all passengers on the train, traveling from Varna to Galatz.\n",
      "\n",
      "I should structure this in the JSON format specified. The \"relations\" array will list each relationship. Each relationship object should have \"source,\" \"target,\" and \"type.\" Since all the people are traveling together, their relationships are just being passengers. \n",
      "\n",
      "So, the relationships would be:\n",
      "1. Van Helsing and Mrs. Harker are traveling together.\n",
      "2. Van Helsing and Quincey are traveling together.\n",
      "3. Van Helsing and Professor are traveling together.\n",
      "4. Mrs. Harker and Quincey are traveling together.\n",
      "5. Mrs. Harker and Professor are traveling together.\n",
      "6. Quincey and Professor are traveling together.\n",
      "\n",
      "Wait, but the user wants the relationships in the context of the train journey. So maybe it's better to list each pair as being passengers. Alternatively, since they're all on the same train, their relationship is just being passengers on the journey. But the example provided in the question shows relationships like \"Friend\" or \"Colleague,\" so perhaps in this case, it's more about being on the same journey.\n",
      "\n",
      "Hmm, the user didn't specify the types of relationships, so maybe it's sufficient to note that they are traveling together. Alternatively, perhaps the relationships are just that they are passengers. Since the example uses \"Friend,\" \"colleague,\" etc., but in this case, they are all on the same journey without specific roles, maybe the type is just \"Traveler.\"\n",
      "\n",
      "But the example uses specific types, so maybe I should stick to that. However, without more context, it's hard to determine the exact relationships. So, I'll assume that all the people mentioned are traveling together, so their relationship is just being passengers on the train.\n",
      "\n",
      "Putting it all together, the JSON should have an \"relations\" array with each pair of people and their type as \"Traveler.\" But since the example uses more specific types, perhaps I should list each pair as \"Traveler.\" Alternatively, maybe the type is just \"Traveler\" for all.\n",
      "\n",
      "I think the safest approach is to list each pair as \"Traveler\" since they are all passengers. So, the JSON would have six relationships, each with the source and target as the people mentioned.\n",
      "</think>\n",
      "\n",
      "```json\n",
      "{\n",
      "  \"relations\": [\n",
      "    {\n",
      "      \"source\": \"Van Helsing\",\n",
      "      \"target\": \"Mrs. Harker\",\n",
      "      \"type\": \"Traveler\"\n",
      "    },\n",
      "    {\n",
      "      \"source\": \"Van Helsing\",\n",
      "      \"target\": \"Quincey\",\n",
      "      \"type\": \"Traveler\"\n",
      "    },\n",
      "    {\n",
      "      \"source\": \"Van Helsing\",\n",
      "      \"target\": \"Professor\",\n",
      "      \"type\": \"Traveler\"\n",
      "    },\n",
      "    {\n",
      "      \"source\": \"Mrs. Harker\",\n",
      "      \"target\": \"Quincey\",\n",
      "      \"type\": \"Traveler\"\n",
      "    },\n",
      "    {\n",
      "      \"source\": \"Mrs. Harker\",\n",
      "      \"target\": \"Professor\",\n",
      "      \"type\": \"Traveler\"\n",
      "    },\n",
      "    {\n",
      "      \"source\": \"Quincey\",\n",
      "      \"target\": \"Professor\",\n",
      "      \"type\": \"Traveler\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "```\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['\\n{\\n  \"relations\": [\\n    {\\n      \"source\": \"Van Helsing\",\\n      \"target\": \"Mrs. Harker\",\\n      \"type\": \"Traveler\"\\n    },\\n    {\\n      \"source\": \"Van Helsing\",\\n      \"target\": \"Quincey\",\\n      \"type\": \"Traveler\"\\n    },\\n    {\\n      \"source\": \"Van Helsing\",\\n      \"target\": \"Professor\",\\n      \"type\": \"Traveler\"\\n    },\\n    {\\n      \"source\": \"Mrs. Harker\",\\n      \"target\": \"Quincey\",\\n      \"type\": \"Traveler\"\\n    },\\n    {\\n      \"source\": \"Mrs. Harker\",\\n      \"target\": \"Professor\",\\n      \"type\": \"Traveler\"\\n    },\\n    {\\n      \"source\": \"Quincey\",\\n      \"target\": \"Professor\",\\n      \"type\": \"Traveler\"\\n    }\\n  ]\\n}\\n']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"assistant\", \"content\": \"You are an expert on reading novels. Read the given context and answer the question of the user.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Find all people and their relationship to each other.\\n\\n{format_instructions}\\n\\n{context}\\n\\n\"}\n",
    "]\n",
    "\n",
    "template = tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "relation_prompt = PromptTemplate(\n",
    "    template=template,\n",
    "    input_variables=[\"context\"],\n",
    "    partial_variables={\"format_instructions\": rel_parser.get_format_instructions()}\n",
    ")\n",
    "\n",
    "print(relation_prompt)\n",
    "\n",
    "relation_chain = relation_prompt | llm.bind(skip_prompt=True)\n",
    "\n",
    "r = relation_chain.invoke(input={\"context\": context})\n",
    "print(r)\n",
    "\n",
    "re.findall(r'```json(.*?)```', r, re.DOTALL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "259b2233-b350-4f4b-a7ed-86878ffb2fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KnowledgeGraphLLM:\n",
    "    def __init__(self):\n",
    "        self.kg = KnowledgeGraph()\n",
    "        self.kg.clear()\n",
    "        self.llm = llm\n",
    "        \n",
    "        self.relation_chain = relation_chain\n",
    "\n",
    "    def extract_relationships(self, context: str) -> List[Dict[str, str]]:\n",
    "        output = self.relation_chain.invoke(input={\"context\": context})\n",
    "        relationships = []\n",
    "        \n",
    "        try:\n",
    "            results = re.findall(r'```json(.*?)```', output, re.DOTALL)\n",
    "            relationships = rel_parser.parse(results)\n",
    "            \n",
    "            if results:\n",
    "                data = json.loads(results[-1])\n",
    "                relationships = data.get(\"relations\", [])\n",
    "\n",
    "                for relation in relationships:\n",
    "                    source = relation[\"source\"]\n",
    "                    target = relation[\"target\"]\n",
    "                    relationship = relation[\"type\"]\n",
    "                    \n",
    "                    self.kg.add_node(source, {\"name\": source})\n",
    "                    self.kg.add_node(target, {\"name\": target})\n",
    "                    self.kg.add_edge(source, target, relationship)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(\"Error: Could not parse LLM response as JSON\\n\", e)\n",
    "            return []\n",
    "\n",
    "        return relationships\n",
    "    \n",
    "    def graph_data(self):\n",
    "        return self.kg.dump()\n",
    "        \n",
    "    def graph_summary(self) -> Dict[str, Any]:\n",
    "        return {\n",
    "            \"node_count\": len(self.kg.nodes),\n",
    "            \"edge_count\": sum(len(edges) for edges in self.kg.edges.values()),\n",
    "            \"data\": self.kg.dump()\n",
    "        }\n",
    "\n",
    "# Example usage\n",
    "def query_llm(kg_llm, contexts, questions):\n",
    "    # Add information through unstructured text\n",
    "    # Extract and add relationships\n",
    "\n",
    "    for i, ctx in enumerate(contexts):\n",
    "        t0 = time.time()\n",
    "\n",
    "        relationships = kg_llm.extract_relationships(ctx)\n",
    "\n",
    "        t1 = time.time()\n",
    "        print(f'Read chunk {i} in {round(t1-t0, 1)} seconds, {round(i / len(contexts) * 100.0, 1)}% done.')\n",
    "        if i % 5 == 0:\n",
    "            print(kg_llm.graph_data())\n",
    "            torch.cuda.empty_cache()\n",
    "            print_mem()            \n",
    "\n",
    "    \n",
    "    # Query the enhanced knowledge graph\n",
    "    for question in questions:\n",
    "        answer = kg_llm.smart_query(question)\n",
    "        print(f\"###\\n\\nQ: {question}\\nA: {answer}\\n\\n###\")\n",
    "    \n",
    "    # Get graph summary\n",
    "    print(\"\\nKnowledge Graph Summary:\")\n",
    "    print(json.dumps(kg_llm.graph_summary(), indent=2))\n",
    "\n",
    "    return kg_llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334ed151-b828-4056-a0c1-e5006b92ef31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_book():\n",
    "    # Initialize the system\n",
    "    kg_llm = KnowledgeGraphLLM()\n",
    "    \n",
    "    with open(\"data/dracula.txt\") as f:\n",
    "        context = f.read()\n",
    "        context = context.rpartition(r\"\\n\\nDRACULA\\n\\n\")[-1]\n",
    "\n",
    "    paragraphs = []\n",
    "    chapters = re.split(r\"\\n\\nCHAPTER\\ .*\\n\", context)\n",
    "    for c in chapters[:5]:\n",
    "        for p in re.split(r\"\\n\\n\", c):\n",
    "            paragraphs.append(p)\n",
    "\n",
    "    questions = [\n",
    "        # \"Who are the main characters in this story?\",\n",
    "        # \"What is their relation to each other?\",\n",
    "        # \"Who is the antagonist of the story?\",\n",
    "        # \"How does the antagonist end up?\"\n",
    "    ]\n",
    "    \n",
    "    query_llm(kg_llm, paragraphs questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57e1c04-184b-4c6c-a776-c9bd4850b66a",
   "metadata": {},
   "outputs": [],
   "source": [
    "parse_book()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef7dcd0-6433-4d5c-9e82-246b42f31b84",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
