{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f6ec52a-c98b-4f31-b0e7-e15d5b9e36a6",
   "metadata": {},
   "source": [
    "# Playground notebook for finding entities and relations\n",
    "\n",
    "- Using REBEL to find entities and their relationships\n",
    "  - This model finds all kinds of entities and their relations, including locations and organisations\n",
    "- Using NER to find entities\n",
    "  - This model finds all kinds of entities, no relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c41a91f-0a48-4467-9f5f-4c67a03cc05c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s><triplet> Punta Cana <subj> La Altagracia Province <obj> located in the administrative territorial entity <subj> Dominican Republic <obj> country <triplet> Higuey <subj> La Altagracia Province <obj> located in the administrative territorial entity <subj> Dominican Republic <obj> country <triplet> La Altagracia Province <subj> Dominican Republic <obj> country <triplet> Dominican Republic <subj> La Altagracia Province <obj> contains administrative territorial entity</s>\n",
      "[{'head': 'Punta Cana', 'type': 'located in the administrative territorial entity', 'tail': 'La Altagracia Province'}, {'head': 'Punta Cana', 'type': 'country', 'tail': 'Dominican Republic'}, {'head': 'Higuey', 'type': 'located in the administrative territorial entity', 'tail': 'La Altagracia Province'}, {'head': 'Higuey', 'type': 'country', 'tail': 'Dominican Republic'}, {'head': 'La Altagracia Province', 'type': 'country', 'tail': 'Dominican Republic'}, {'head': 'Dominican Republic', 'type': 'contains administrative territorial entity', 'tail': 'La Altagracia Province'}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "triplet_extractor = pipeline('text2text-generation', model='Babelscape/rebel-large', tokenizer='Babelscape/rebel-large')\n",
    "# We need to use the tokenizer manually since we need special tokens.\n",
    "extracted_text = triplet_extractor.tokenizer.batch_decode([triplet_extractor(\"Punta Cana is a resort town in the municipality of Higuey, in La Altagracia Province, the eastern most province of the Dominican Republic\", return_tensors=True, return_text=False)[0][\"generated_token_ids\"]])\n",
    "\n",
    "print(extracted_text[0])\n",
    "# Function to parse the generated text and extract the triplets\n",
    "def extract_triplets(text):\n",
    "    triplets = []\n",
    "    relation, subject, relation, object_ = '', '', '', ''\n",
    "    text = text.strip()\n",
    "    current = 'x'\n",
    "    for token in text.replace(\"<s>\", \"\").replace(\"<pad>\", \"\").replace(\"</s>\", \"\").split():\n",
    "        if token == \"<triplet>\":\n",
    "            current = 't'\n",
    "            if relation != '':\n",
    "                triplets.append({'head': subject.strip(), 'type': relation.strip(),'tail': object_.strip()})\n",
    "                relation = ''\n",
    "            subject = ''\n",
    "        elif token == \"<subj>\":\n",
    "            current = 's'\n",
    "            if relation != '':\n",
    "                triplets.append({'head': subject.strip(), 'type': relation.strip(),'tail': object_.strip()})\n",
    "            object_ = ''\n",
    "        elif token == \"<obj>\":\n",
    "            current = 'o'\n",
    "            relation = ''\n",
    "        else:\n",
    "            if current == 't':\n",
    "                subject += ' ' + token\n",
    "            elif current == 's':\n",
    "                object_ += ' ' + token\n",
    "            elif current == 'o':\n",
    "                relation += ' ' + token\n",
    "    if subject != '' and relation != '' and object_ != '':\n",
    "        triplets.append({'head': subject.strip(), 'type': relation.strip(),'tail': object_.strip()})\n",
    "    return triplets\n",
    "extracted_triplets = extract_triplets(extracted_text[0])\n",
    "print(extracted_triplets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a3a8d36c-339a-4942-b217-9dc2a2ff08ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dslim/bert-base-NER were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/opt/conda/lib/python3.11/site-packages/transformers/pipelines/token_classification.py:168: UserWarning: `grouped_entities` is deprecated and will be removed in version v5.0.0, defaulted to `aggregation_strategy=\"AggregationStrategy.SIMPLE\"` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import pipeline\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"dslim/bert-base-NER\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"dslim/bert-base-NER\")\n",
    "\n",
    "nlp = pipeline(\"ner\", model=model, tokenizer=tokenizer, grouped_entities=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a69b28f7-cfba-4995-a597-3bd239c537d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13064\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "with open(\"data/dracula.txt\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "paragraphs = re.split(r'[\\n]{2,}', text)\n",
    "len(paragraphs)\n",
    "\n",
    "lines = []\n",
    "for p in paragraphs:\n",
    "    lines.extend(re.split(r'[\\n]{1,}', p))\n",
    "\n",
    "print(len(lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca2b95d8-8b17-429e-ab56-fcff5b9296d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed: 0 lines\n",
      "Processed: 1000 lines\n",
      "Processed: 2000 lines\n",
      "Processed: 3000 lines\n",
      "Processed: 4000 lines\n",
      "Processed: 5000 lines\n",
      "Processed: 6000 lines\n",
      "Processed: 7000 lines\n",
      "Processed: 8000 lines\n",
      "Processed: 9000 lines\n",
      "Processed: 10000 lines\n",
      "Processed: 11000 lines\n",
      "Processed: 12000 lines\n",
      "Processed: 13000 lines\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "314"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entities = {}\n",
    "for i, line in enumerate(lines):\n",
    "    if (i % 1000 == 0):\n",
    "        print(f\"Processed: {i} lines\")\n",
    "    \n",
    "    results = nlp(line)\n",
    "    for result in results:\n",
    "        word = line[result[\"start\"]:result[\"end\"]]\n",
    "        entity_group = result[\"entity_group\"]\n",
    "        score = result[\"score\"]\n",
    "        \n",
    "        if entity_group == \"PER\":\n",
    "            dummy = {}\n",
    "            dummy.setdefault(\"score\", 0)\n",
    "            dummy.setdefault(\"count\", 0)\n",
    "            \n",
    "            entities.setdefault(word, dummy)\n",
    "            entry = entities[word]\n",
    "            \n",
    "            if score > entry[\"score\"]:\n",
    "                entry[\"score\"] = round(score, 3)\n",
    "                entry[\"type\"] = entity_group\n",
    "                entry[\"count\"] += 1\n",
    "    \n",
    "                entities[word] = entry\n",
    "\n",
    "len(entities)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
